{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e599fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from wgan import FE, Discriminator, Classifier, Wasserstein_Loss, Grad_Loss\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "from train_val_model import printsave, train_val, make_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13ff8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hun/바탕화면/AAI/DRDA_Wasserstein_Loss'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb38aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = open(\"log.txt\", \"w\")\n",
    "print(\"DRDA_Wasserstein_Loss\\n\", file=log)\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6901c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data path check\n",
      "s05.mat s08.mat s17.mat s07.mat s06.mat s27.mat s22.mat s10.mat s25.mat s20.mat s16.mat s15.mat s21.mat s03.mat s02.mat s31.mat s11.mat s12.mat s32.mat s13.mat s29.mat s09.mat s28.mat s01.mat s30.mat s14.mat s19.mat s26.mat s04.mat s18.mat s24.mat s23.mat "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read data: 100%|██████████| 32/32 [00:20<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# path\n",
    "path = r'../data_preprocessed_matlab/'  # 경로는 저장 파일 경로\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "printsave(\"data path check\")\n",
    "for i in file_list:    # 확인\n",
    "    printsave(i, end=' ')\n",
    "\n",
    "\n",
    "for i in tqdm(file_list, desc=\"read data\"): \n",
    "    mat_file = io.loadmat(path+i)\n",
    "    data = mat_file['data']\n",
    "    labels = np.array(mat_file['labels'])\n",
    "    val = labels.T[0].round().astype(np.int8)\n",
    "    aro = labels.T[1].round().astype(np.int8)\n",
    "    \n",
    "    if(i==\"s05.mat\"): \n",
    "        Data = data\n",
    "        VAL = val\n",
    "        ARO = aro\n",
    "        continue\n",
    "        \n",
    "    Data = np.concatenate((Data ,data),axis=0)   # 밑으로 쌓아서 하나로 만듬\n",
    "    VAL = np.concatenate((VAL ,val),axis=0)\n",
    "    ARO = np.concatenate((ARO ,aro),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb07831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocess channel: 100%|██████████| 1280/1280 [00:00<00:00, 123248.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# eeg preprocessing\n",
    "\n",
    "eeg_data = []\n",
    "peripheral_data = []\n",
    "\n",
    "for i in tqdm(range(len(Data)), desc=\"preprocess channel\"):\n",
    "    for j in range (40): \n",
    "        if(j < 32): # get channels 1 to 32\n",
    "            eeg_data.append(Data[i][j])\n",
    "        else:\n",
    "            peripheral_data.append(Data[i][j])\n",
    "\n",
    "# set data type, shape\n",
    "eeg_data = np.reshape(eeg_data, (len(Data),1,32, 8064))\n",
    "eeg_data = eeg_data.astype('float32')\n",
    "eeg_data32 = torch.from_numpy(eeg_data)\n",
    "VAL = (torch.from_numpy(VAL)).type(torch.long)\n",
    "ARO = (torch.from_numpy(ARO)).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7317b521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data split\n",
      "make data loader\n",
      "data split\n",
      "make data loader\n"
     ]
    }
   ],
   "source": [
    "source_VAL, target_VAL, val_VAL = make_dataloader(eeg_data32, VAL)\n",
    "source_ARO, target_ARO, val_ARO = make_dataloader(eeg_data32, ARO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d973c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "VALENCE label train and valiation\n",
      "device:  cuda:0\n",
      "\n",
      "1 : epoch\n",
      "\n",
      "gloss : -63.14057752821181\n",
      "wd_loss : 1.0227751202053494\n",
      "cls_loss : 2.1742867363823786\n",
      "acc_t : 0.10243055555555555\n",
      "acc_s : 0.24016202820671928\n",
      "val_loss : 2.19820032119751\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  1  9  0  0  0]\n",
      " [ 0  1  0  0  1 14  0  0  0]\n",
      " [ 0  0  0  0  3 14  0  0  0]\n",
      " [ 0  0  0  0  3 25  0  1  0]\n",
      " [ 0  0  0  0  3 12  0  0  1]\n",
      " [ 0  1  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  2 17  0  0  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.20      0.10      0.14        29\n",
      "           6       0.09      0.75      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.03      0.09      0.03       160\n",
      "weighted avg       0.04      0.09      0.04       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "2 : epoch\n",
      "\n",
      "gloss : -22.977615356445312\n",
      "wd_loss : 2.4704411824544272\n",
      "cls_loss : 2.1617550320095487\n",
      "acc_t : 0.10185185405943128\n",
      "acc_s : 0.3043981393178304\n",
      "val_loss : 2.19931697845459\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  1  9  0  0  0]\n",
      " [ 0  0  0  0  0 16  0  0  0]\n",
      " [ 0  0  0  0  1 16  0  0  0]\n",
      " [ 0  0  0  0  3 26  0  0  0]\n",
      " [ 0  0  0  0  1 14  0  0  1]\n",
      " [ 0  1  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  2 17  0  0  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.30      0.10      0.15        29\n",
      "           6       0.09      0.88      0.17        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.04      0.11      0.04       160\n",
      "weighted avg       0.06      0.11      0.04       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "3 : epoch\n",
      "\n",
      "gloss : 5.964959038628472\n",
      "wd_loss : 4.7177145216200085\n",
      "cls_loss : 2.1516420576307507\n",
      "acc_t : 0.10011574294832018\n",
      "acc_s : 0.359375\n",
      "val_loss : 2.200141096115112\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  0  0  0  2 14  0  0  0]\n",
      " [ 0  0  0  0  2 15  0  0  0]\n",
      " [ 0  0  0  0  4 25  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  1]\n",
      " [ 0  0  0  0  0 38  0  0  0]\n",
      " [ 0  0  0  0  1 17  0  0  1]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.31      0.14      0.19        29\n",
      "           6       0.10      0.94      0.19        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.05      0.12      0.04       160\n",
      "weighted avg       0.07      0.12      0.05       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "4 : epoch\n",
      "\n",
      "gloss : 64.10796440972223\n",
      "wd_loss : 8.386525472005209\n",
      "cls_loss : 2.1432969835069446\n",
      "acc_t : 0.1076388888888889\n",
      "acc_s : 0.4143518606821696\n",
      "val_loss : 2.20019793510437\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  1  0  1  1 12  0  0  1]\n",
      " [ 0  0  0  1  2 14  0  0  0]\n",
      " [ 0  0  0  0  4 25  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  1]\n",
      " [ 0  0  0  0  1 37  0  0  0]\n",
      " [ 0  0  0  0  2 16  0  0  1]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.50      0.06      0.11        17\n",
      "           5       0.29      0.14      0.19        29\n",
      "           6       0.11      0.94      0.19        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.10      0.13      0.05       160\n",
      "weighted avg       0.12      0.12      0.06       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "5 : epoch\n",
      "\n",
      "gloss : -393.3486328125\n",
      "wd_loss : 12.78656005859375\n",
      "cls_loss : 2.1390868292914496\n",
      "acc_t : 0.10185185405943128\n",
      "acc_s : 0.45717594358656144\n",
      "val_loss : 2.1992982387542725\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  1  8  0  0  1]\n",
      " [ 0  1  0  1  2 12  0  0  0]\n",
      " [ 0  0  0  1  3 13  0  0  0]\n",
      " [ 0  0  0  0  3 26  0  0  0]\n",
      " [ 0  0  0  0  1 14  0  0  1]\n",
      " [ 0  0  1  0  1 36  0  0  0]\n",
      " [ 0  0  0  0  2 16  0  0  1]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.50      0.06      0.11        17\n",
      "           5       0.20      0.10      0.14        29\n",
      "           6       0.10      0.88      0.18        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.09      0.12      0.05       160\n",
      "weighted avg       0.10      0.11      0.05       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "6 : epoch\n",
      "\n",
      "gloss : -1732.5516493055557\n",
      "wd_loss : 18.51106940375434\n",
      "cls_loss : 2.1351721021864147\n",
      "acc_t : 0.09432870811886257\n",
      "acc_s : 0.4849537213643392\n",
      "val_loss : 2.198627758026123\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  1  1  8  0  0  0]\n",
      " [ 0  1  0  1  2 12  0  0  0]\n",
      " [ 0  0  0  2  3 12  0  0  0]\n",
      " [ 0  0  0  0  3 26  0  0  0]\n",
      " [ 0  0  0  0  1 13  0  0  2]\n",
      " [ 0  0  1  0  1 36  0  0  0]\n",
      " [ 0  0  0  0  4 14  0  0  1]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.50      0.12      0.19        17\n",
      "           5       0.19      0.10      0.13        29\n",
      "           6       0.10      0.81      0.17        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.09      0.11      0.06       160\n",
      "weighted avg       0.10      0.11      0.06       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "7 : epoch\n",
      "\n",
      "gloss : -8391.953125\n",
      "wd_loss : 26.157202826605904\n",
      "cls_loss : 2.128426021999783\n",
      "acc_t : 0.1076388888888889\n",
      "acc_s : 0.502893500857883\n",
      "val_loss : 2.1990684509277343\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  8  0  0  0]\n",
      " [ 0  0  0  1  1  8  0  0  0]\n",
      " [ 0  1  0  1  2 12  0  0  0]\n",
      " [ 0  0  0  2  3 12  0  0  0]\n",
      " [ 0  0  0  0  3 26  0  0  0]\n",
      " [ 0  0  0  1  1 13  0  0  1]\n",
      " [ 0  0  1  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  3 16  0  0  0]\n",
      " [ 0  1  0  0  1  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.40      0.12      0.18        17\n",
      "           5       0.21      0.10      0.14        29\n",
      "           6       0.09      0.81      0.17        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.08      0.11      0.05       160\n",
      "weighted avg       0.09      0.11      0.06       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "8 : epoch\n",
      "\n",
      "gloss : -16957.824652777777\n",
      "wd_loss : 33.672088623046875\n",
      "cls_loss : 2.1248486836751304\n",
      "acc_t : 0.10821759038501316\n",
      "acc_s : 0.5196759435865614\n",
      "val_loss : 2.199499320983887\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  1  0  2  3 10  0  0  0]\n",
      " [ 0  0  0  1  3 13  0  0  0]\n",
      " [ 0  0  0  0  4 25  0  0  0]\n",
      " [ 0  0  0  1  1 14  0  0  0]\n",
      " [ 0  0  1  0  1 36  0  0  0]\n",
      " [ 0  0  0  0  4 15  0  0  0]\n",
      " [ 0  1  0  0  1  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.25      0.06      0.10        17\n",
      "           5       0.20      0.14      0.16        29\n",
      "           6       0.11      0.88      0.19        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.06      0.12      0.05       160\n",
      "weighted avg       0.07      0.12      0.06       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "9 : epoch\n",
      "\n",
      "gloss : -27205.991319444445\n",
      "wd_loss : 43.08251274956597\n",
      "cls_loss : 2.1213099161783853\n",
      "acc_t : 0.10995370149612427\n",
      "acc_s : 0.5445601675245497\n",
      "val_loss : 2.198935079574585\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  1  0  1  3 11  0  0  0]\n",
      " [ 0  0  0  2  4 11  0  0  0]\n",
      " [ 0  1  0  0  2 26  0  0  0]\n",
      " [ 0  0  0  1  4 11  0  0  0]\n",
      " [ 0  0  1  0  3 34  0  0  0]\n",
      " [ 0  0  0  0  4 15  0  0  0]\n",
      " [ 0  1  0  0  0  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.50      0.12      0.19        17\n",
      "           5       0.09      0.07      0.08        29\n",
      "           6       0.09      0.69      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.07      0.10      0.05       160\n",
      "weighted avg       0.08      0.09      0.05       160\n",
      "\n",
      "best_val_loss : 2.19820032119751, epoch : 1\n",
      "\n",
      "10 : epoch\n",
      "\n",
      "gloss : -77865.95138888889\n",
      "wd_loss : 53.129984537760414\n",
      "cls_loss : 2.1189102596706815\n",
      "acc_t : 0.11863426367441814\n",
      "acc_s : 0.5607638888888888\n",
      "val_loss : 2.1978604316711428\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  0  0  1  3 12  0  0  0]\n",
      " [ 0  0  0  2  4 11  0  0  0]\n",
      " [ 0  1  0  0  3 25  0  0  0]\n",
      " [ 0  1  0  0  4 10  0  1  0]\n",
      " [ 0  0  1  0  3 34  0  0  0]\n",
      " [ 0  0  0  0  4 15  0  0  0]\n",
      " [ 0  1  0  0  0  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.67      0.12      0.20        17\n",
      "           5       0.12      0.10      0.11        29\n",
      "           6       0.08      0.62      0.14        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.10      0.09      0.05       160\n",
      "weighted avg       0.10      0.09      0.06       160\n",
      "\n",
      "best_val_loss : 2.1978604316711428, epoch : 10\n",
      "\n",
      "11 : epoch\n",
      "\n",
      "gloss : -134944.70833333334\n",
      "wd_loss : 64.75632731119792\n",
      "cls_loss : 2.1154121822781033\n",
      "acc_t : 0.11053240961498684\n",
      "acc_s : 0.5763888888888888\n",
      "val_loss : 2.1977025985717775\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  7  1  0  0]\n",
      " [ 0  0  0  0  1  9  0  0  0]\n",
      " [ 0  0  0  1  2 13  0  0  0]\n",
      " [ 0  0  0  2  4 11  0  0  0]\n",
      " [ 0  1  0  0  4 24  0  0  0]\n",
      " [ 0  1  0  2  3 10  0  0  0]\n",
      " [ 0  0  1  0  3 34  0  0  0]\n",
      " [ 0  0  0  0  2 16  1  0  0]\n",
      " [ 0  1  0  0  0  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.40      0.12      0.18        17\n",
      "           5       0.21      0.14      0.17        29\n",
      "           6       0.08      0.62      0.14        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.10       160\n",
      "   macro avg       0.08      0.10      0.05       160\n",
      "weighted avg       0.09      0.10      0.06       160\n",
      "\n",
      "best_val_loss : 2.1977025985717775, epoch : 11\n",
      "\n",
      "12 : epoch\n",
      "\n",
      "gloss : -204751.18055555556\n",
      "wd_loss : 76.54147677951389\n",
      "cls_loss : 2.1120071411132812\n",
      "acc_t : 0.10416666666666667\n",
      "acc_s : 0.59375\n",
      "val_loss : 2.1965169429779055\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  7  1  0  0]\n",
      " [ 0  0  0  0  1  9  0  0  0]\n",
      " [ 0  0  0  1  1 14  0  0  0]\n",
      " [ 0  0  0  2  5 10  0  0  0]\n",
      " [ 0  1  0  0  4 24  0  0  0]\n",
      " [ 0  1  0  1  4  9  0  1  0]\n",
      " [ 0  0  1  0  4 33  0  0  0]\n",
      " [ 0  0  0  1  4 13  1  0  0]\n",
      " [ 0  1  0  0  0  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.40      0.12      0.18        17\n",
      "           5       0.17      0.14      0.15        29\n",
      "           6       0.07      0.56      0.13        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.07      0.09      0.05       160\n",
      "weighted avg       0.08      0.09      0.06       160\n",
      "\n",
      "best_val_loss : 2.1965169429779055, epoch : 12\n",
      "\n",
      "13 : epoch\n",
      "\n",
      "gloss : -398848.69444444444\n",
      "wd_loss : 92.10274251302083\n",
      "cls_loss : 2.110285441080729\n",
      "acc_t : 0.10069444444444445\n",
      "acc_s : 0.5972222222222222\n",
      "val_loss : 2.1960318088531494\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  7  1  0  0]\n",
      " [ 0  0  0  0  1  9  0  0  0]\n",
      " [ 0  0  0  1  1 14  0  0  0]\n",
      " [ 0  0  0  2  3 12  0  0  0]\n",
      " [ 0  1  0  0  4 24  0  0  0]\n",
      " [ 0  0  0  1  5  8  1  1  0]\n",
      " [ 0  0  1  0  3 34  0  0  0]\n",
      " [ 0  0  0  1  3 13  1  1  0]\n",
      " [ 0  0  1  0  0  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.40      0.12      0.18        17\n",
      "           5       0.20      0.14      0.16        29\n",
      "           6       0.06      0.50      0.11        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.50      0.05      0.10        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.13      0.09      0.06       160\n",
      "weighted avg       0.14      0.09      0.07       160\n",
      "\n",
      "best_val_loss : 2.1960318088531494, epoch : 13\n",
      "\n",
      "14 : epoch\n",
      "\n",
      "gloss : -486870.27777777775\n",
      "wd_loss : 106.08721245659723\n",
      "cls_loss : 2.1113018459743924\n",
      "acc_t : 0.10185185405943128\n",
      "acc_s : 0.6082176102532281\n",
      "val_loss : 2.1956127166748045\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  7  1  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  0  0  1  2 13  0  0  0]\n",
      " [ 0  1  0  1  6  8  1  0  0]\n",
      " [ 0  1  1  0  5 22  0  0  0]\n",
      " [ 0  0  0  1  3 10  1  1  0]\n",
      " [ 0  1  1  0  5 31  0  0  0]\n",
      " [ 0  0  1  1  3 13  0  1  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.25      0.06      0.10        17\n",
      "           5       0.19      0.17      0.18        29\n",
      "           6       0.08      0.62      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.50      0.05      0.10        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.11      0.10      0.06       160\n",
      "weighted avg       0.13      0.11      0.07       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "15 : epoch\n",
      "\n",
      "gloss : -767585.1111111111\n",
      "wd_loss : 122.87430826822917\n",
      "cls_loss : 2.1074746449788413\n",
      "acc_t : 0.12731481922997367\n",
      "acc_s : 0.6493055555555556\n",
      "val_loss : 2.1957603454589845\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  7  1  0  0]\n",
      " [ 0  0  0  0  3  7  0  0  0]\n",
      " [ 0  0  0  1  2 13  0  0  0]\n",
      " [ 0  0  0  2  5 10  0  0  0]\n",
      " [ 0  0  1  0  6 22  0  0  0]\n",
      " [ 0  0  0  1  3 10  1  1  0]\n",
      " [ 0  1  1  0  7 29  0  0  0]\n",
      " [ 0  1  1  2  4 11  0  0  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.12      0.17        17\n",
      "           5       0.19      0.21      0.20        29\n",
      "           6       0.09      0.62      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.07      0.11      0.06       160\n",
      "weighted avg       0.08      0.11      0.07       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "16 : epoch\n",
      "\n",
      "gloss : -1112450.7777777778\n",
      "wd_loss : 139.83303493923611\n",
      "cls_loss : 2.105240715874566\n",
      "acc_t : 0.11979166666666667\n",
      "acc_s : 0.6574073897467719\n",
      "val_loss : 2.1959948539733887\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  7  1  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  0  0  1  2 13  0  0  0]\n",
      " [ 0  1  0  1  4 10  1  0  0]\n",
      " [ 0  0  1  0  4 24  0  0  0]\n",
      " [ 0  0  0  1  3 10  1  1  0]\n",
      " [ 0  1  1  0  4 32  0  0  0]\n",
      " [ 0  1  1  1  2 12  0  2  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.25      0.06      0.10        17\n",
      "           5       0.17      0.14      0.15        29\n",
      "           6       0.08      0.62      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.67      0.11      0.18        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.13      0.10      0.06       160\n",
      "weighted avg       0.15      0.11      0.07       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "17 : epoch\n",
      "\n",
      "gloss : -2060014.2222222222\n",
      "wd_loss : 162.28910319010416\n",
      "cls_loss : 2.10501586066352\n",
      "acc_t : 0.10995370149612427\n",
      "acc_s : 0.6637731658087836\n",
      "val_loss : 2.197000741958618\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  0  7  1  0  0]\n",
      " [ 0  0  0  0  3  7  0  0  0]\n",
      " [ 0  0  0  0  3 12  1  0  0]\n",
      " [ 0  1  0  1  5 10  0  0  0]\n",
      " [ 0  0  1  0  3 24  1  0  0]\n",
      " [ 0  0  0  1  3 10  1  1  0]\n",
      " [ 0  1  1  0  5 31  0  0  0]\n",
      " [ 0  0  2  2  0 13  1  1  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.25      0.06      0.10        17\n",
      "           5       0.12      0.10      0.11        29\n",
      "           6       0.08      0.62      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.50      0.05      0.10        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.11      0.09      0.05       160\n",
      "weighted avg       0.12      0.09      0.06       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "18 : epoch\n",
      "\n",
      "gloss : -2884457.777777778\n",
      "wd_loss : 186.63812934027777\n",
      "cls_loss : 2.099466747707791\n",
      "acc_t : 0.1134259303410848\n",
      "acc_s : 0.6938657230801053\n",
      "val_loss : 2.196922492980957\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  7  0  0  0]\n",
      " [ 0  0  0  0  3  7  0  0  0]\n",
      " [ 0  0  0  0  3 12  1  0  0]\n",
      " [ 0  1  0  1  6  9  0  0  0]\n",
      " [ 0  0  1  0  5 22  1  0  0]\n",
      " [ 0  0  0  0  4 10  1  1  0]\n",
      " [ 0  1  1  0  8 28  0  0  0]\n",
      " [ 0  0  2  2  2 12  1  0  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.06      0.10        17\n",
      "           5       0.15      0.17      0.16        29\n",
      "           6       0.09      0.62      0.16        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.10       160\n",
      "   macro avg       0.06      0.10      0.05       160\n",
      "weighted avg       0.07      0.10      0.06       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "19 : epoch\n",
      "\n",
      "gloss : -4632936.0\n",
      "wd_loss : 210.01447211371527\n",
      "cls_loss : 2.1025909847683377\n",
      "acc_t : 0.11805555555555555\n",
      "acc_s : 0.6597222222222222\n",
      "val_loss : 2.198230266571045\n",
      "confusion_matrix\n",
      "[[ 0  1  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  0  0  0  3 12  1  0  0]\n",
      " [ 0  0  0  1  6 10  0  0  0]\n",
      " [ 0  0  1  0  4 23  1  0  0]\n",
      " [ 0  0  1  0  1 12  1  1  0]\n",
      " [ 0  1  0  0  5 32  0  0  0]\n",
      " [ 0  0  3  2  1 12  1  0  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.06      0.10        17\n",
      "           5       0.17      0.14      0.15        29\n",
      "           6       0.10      0.75      0.18        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.07      0.11      0.05       160\n",
      "weighted avg       0.08      0.11      0.06       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "20 : epoch\n",
      "\n",
      "gloss : -5138559.555555556\n",
      "wd_loss : 237.00675455729166\n",
      "cls_loss : 2.1002364688449435\n",
      "acc_t : 0.1267361111111111\n",
      "acc_s : 0.6990740564134386\n",
      "val_loss : 2.198749542236328\n",
      "confusion_matrix\n",
      "[[ 0  1  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  0  1  0  4 11  0  0  0]\n",
      " [ 1  0  0  1  6  9  0  0  0]\n",
      " [ 0  0  2  0  5 21  1  0  0]\n",
      " [ 0  0  1  0  3 10  1  1  0]\n",
      " [ 0  1  2  0  6 29  0  0  0]\n",
      " [ 0  0  4  2  2 11  0  0  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.10      0.06      0.08        16\n",
      "           4       0.33      0.06      0.10        17\n",
      "           5       0.17      0.17      0.17        29\n",
      "           6       0.09      0.62      0.16        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.08      0.10      0.06       160\n",
      "weighted avg       0.08      0.11      0.06       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "21 : epoch\n",
      "\n",
      "gloss : -6711685.333333333\n",
      "wd_loss : 263.2033962673611\n",
      "cls_loss : 2.097042719523112\n",
      "acc_t : 0.13194444444444445\n",
      "acc_s : 0.7164351675245497\n",
      "val_loss : 2.1981717109680177\n",
      "confusion_matrix\n",
      "[[ 0  1  0  0  0  7  0  0  0]\n",
      " [ 0  0  0  0  3  7  0  0  0]\n",
      " [ 0  0  0  0  5 11  0  0  0]\n",
      " [ 1  0  0  1  7  8  0  0  0]\n",
      " [ 0  0  1  0  4 22  1  0  1]\n",
      " [ 0  0  0  0  5  9  1  1  0]\n",
      " [ 0  0  1  0  6 30  0  1  0]\n",
      " [ 0  0  4  2  2 11  0  0  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.06      0.10        17\n",
      "           5       0.12      0.14      0.13        29\n",
      "           6       0.08      0.56      0.14        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.06      0.08      0.04       160\n",
      "weighted avg       0.06      0.09      0.05       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "22 : epoch\n",
      "\n",
      "gloss : -8354790.222222222\n",
      "wd_loss : 299.2236056857639\n",
      "cls_loss : 2.0957698822021484\n",
      "acc_t : 0.1388888888888889\n",
      "acc_s : 0.7025462786356608\n",
      "val_loss : 2.198546314239502\n",
      "confusion_matrix\n",
      "[[ 0  1  0  0  1  6  0  0  0]\n",
      " [ 0  0  1  0  2  7  0  0  0]\n",
      " [ 0  0  1  1  3 11  0  0  0]\n",
      " [ 1  0  0  1  5  9  0  0  1]\n",
      " [ 0  0  0  0  4 23  1  0  1]\n",
      " [ 0  0  0  0  5  9  1  1  0]\n",
      " [ 0  1  1  0  4 31  0  1  0]\n",
      " [ 0  0  2  2  2 13  0  0  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.20      0.06      0.10        16\n",
      "           4       0.25      0.06      0.10        17\n",
      "           5       0.14      0.14      0.14        29\n",
      "           6       0.08      0.56      0.14        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.07      0.09      0.05       160\n",
      "weighted avg       0.08      0.09      0.06       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "23 : epoch\n",
      "\n",
      "gloss : -15911584.0\n",
      "wd_loss : 336.7995334201389\n",
      "cls_loss : 2.093017154269748\n",
      "acc_t : 0.13194444444444445\n",
      "acc_s : 0.7222222222222222\n",
      "val_loss : 2.19864559173584\n",
      "confusion_matrix\n",
      "[[ 0  1  0  0  2  5  0  0  0]\n",
      " [ 0  0  1  1  2  6  0  0  0]\n",
      " [ 0  1  1  0  3 11  0  0  0]\n",
      " [ 1  0  0  1  5  9  0  0  1]\n",
      " [ 0  0  1  0  4 23  1  0  0]\n",
      " [ 0  0  0  0  5  9  1  1  0]\n",
      " [ 0  1  3  0  5 28  0  1  0]\n",
      " [ 0  0  3  2  3 11  0  0  0]\n",
      " [ 0  0  0  0  2  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.11      0.06      0.08        16\n",
      "           4       0.25      0.06      0.10        17\n",
      "           5       0.13      0.14      0.13        29\n",
      "           6       0.08      0.56      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.09       160\n",
      "   macro avg       0.06      0.09      0.05       160\n",
      "weighted avg       0.07      0.09      0.06       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "24 : epoch\n",
      "\n",
      "gloss : -14985953.777777778\n",
      "wd_loss : 370.78843858506946\n",
      "cls_loss : 2.094825108846029\n",
      "acc_t : 0.1423611111111111\n",
      "acc_s : 0.715856499142117\n",
      "val_loss : 2.2006480216979982\n",
      "confusion_matrix\n",
      "[[ 0  1  0  0  2  5  0  0  0]\n",
      " [ 0  0  0  1  2  7  0  0  0]\n",
      " [ 0  0  1  0  3 11  0  1  0]\n",
      " [ 0  0  0  1  5 11  0  0  0]\n",
      " [ 0  1  0  0  2 25  1  0  0]\n",
      " [ 0  0  0  0  4 10  1  1  0]\n",
      " [ 0  1  1  0  5 30  0  1  0]\n",
      " [ 0  0  3  1  1 12  0  2  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.20      0.06      0.10        16\n",
      "           4       0.33      0.06      0.10        17\n",
      "           5       0.08      0.07      0.08        29\n",
      "           6       0.08      0.62      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.40      0.11      0.17        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.10       160\n",
      "   macro avg       0.12      0.10      0.07       160\n",
      "weighted avg       0.13      0.10      0.07       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "25 : epoch\n",
      "\n",
      "gloss : -26884819.555555556\n",
      "wd_loss : 407.181396484375\n",
      "cls_loss : 2.0872008005777993\n",
      "acc_t : 0.12962962521447075\n",
      "acc_s : 0.734375\n",
      "val_loss : 2.2021934032440185\n",
      "confusion_matrix\n",
      "[[ 0  1  0  1  1  5  0  0  0]\n",
      " [ 0  0  0  1  2  6  0  1  0]\n",
      " [ 0  0  1  1  3 10  0  1  0]\n",
      " [ 0  1  0  1  8  7  0  0  0]\n",
      " [ 0  1  0  0  4 23  1  0  0]\n",
      " [ 0  0  0  1  3 10  1  1  0]\n",
      " [ 0  1  1  0  7 29  0  0  0]\n",
      " [ 0  0  4  1  4  8  0  2  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.17      0.06      0.09        16\n",
      "           4       0.17      0.06      0.09        17\n",
      "           5       0.12      0.14      0.13        29\n",
      "           6       0.10      0.62      0.17        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.40      0.11      0.17        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.11      0.11      0.07       160\n",
      "weighted avg       0.11      0.11      0.08       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "26 : epoch\n",
      "\n",
      "gloss : -36213450.666666664\n",
      "wd_loss : 447.12217881944446\n",
      "cls_loss : 2.0875937143961587\n",
      "acc_t : 0.13368055555555555\n",
      "acc_s : 0.7401620546976725\n",
      "val_loss : 2.2005236625671385\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  1  6  0  0  0]\n",
      " [ 0  0  1  0  1  8  0  0  0]\n",
      " [ 0  1  0  0  3 12  0  0  0]\n",
      " [ 0  0  0  1  8  8  0  0  0]\n",
      " [ 0  0  2  0  2 24  1  0  0]\n",
      " [ 0  0  0  1  3 10  1  1  0]\n",
      " [ 0  1  2  0  5 30  0  0  0]\n",
      " [ 0  0  4  0  2 10  0  3  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.06      0.10        17\n",
      "           5       0.08      0.07      0.07        29\n",
      "           6       0.09      0.62      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.75      0.16      0.26        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.10       160\n",
      "   macro avg       0.14      0.10      0.07       160\n",
      "weighted avg       0.15      0.10      0.07       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "27 : epoch\n",
      "\n",
      "gloss : -51690129.777777776\n",
      "wd_loss : 486.6111111111111\n",
      "cls_loss : 2.089860280354818\n",
      "acc_t : 0.11805555555555555\n",
      "acc_s : 0.7274305555555556\n",
      "val_loss : 2.1999715328216554\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  1  6  0  0  0]\n",
      " [ 0  0  0  0  2  8  0  0  0]\n",
      " [ 0  1  0  0  2 13  0  0  0]\n",
      " [ 0  0  0  1  9  7  0  0  0]\n",
      " [ 0  0  2  0  2 24  1  0  0]\n",
      " [ 0  0  0  1  1 12  1  1  0]\n",
      " [ 0  1  2  0  6 29  0  0  0]\n",
      " [ 0  0  4  0  1 10  0  4  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.06      0.10        17\n",
      "           5       0.08      0.07      0.07        29\n",
      "           6       0.10      0.75      0.18        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.80      0.21      0.33        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.15      0.12      0.08       160\n",
      "weighted avg       0.16      0.12      0.08       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "28 : epoch\n",
      "\n",
      "gloss : -48459363.55555555\n",
      "wd_loss : 528.4794921875\n",
      "cls_loss : 2.0846640268961587\n",
      "acc_t : 0.13541666666666666\n",
      "acc_s : 0.7401620546976725\n",
      "val_loss : 2.199639844894409\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  3  4  0  0  0]\n",
      " [ 0  0  1  0  1  7  0  1  0]\n",
      " [ 0  1  0  1  2 11  0  1  0]\n",
      " [ 0  0  0  1  7  8  0  0  1]\n",
      " [ 0  0  2  0  8 18  1  0  0]\n",
      " [ 0  1  0  1  2 10  1  1  0]\n",
      " [ 0  1  1  0 14 22  0  0  0]\n",
      " [ 0  0  4  0  5  6  0  4  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.25      0.06      0.10        17\n",
      "           5       0.19      0.28      0.22        29\n",
      "           6       0.11      0.62      0.19        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.57      0.21      0.31        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.12      0.13      0.09       160\n",
      "weighted avg       0.14      0.14      0.11       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "29 : epoch\n",
      "\n",
      "gloss : -88042808.8888889\n",
      "wd_loss : 586.6849500868055\n",
      "cls_loss : 2.0827789306640625\n",
      "acc_t : 0.13599537478552925\n",
      "acc_s : 0.7690972222222222\n",
      "val_loss : 2.1962326526641847\n",
      "confusion_matrix\n",
      "[[ 0  0  0  2  2  4  0  0  0]\n",
      " [ 0  0  1  0  2  6  0  1  0]\n",
      " [ 0  0  0  2  2 11  0  1  0]\n",
      " [ 0  0  0  1  3 12  0  0  1]\n",
      " [ 0  0  1  1  4 22  1  0  0]\n",
      " [ 0  0  0  2  0 10  1  3  0]\n",
      " [ 0  1  1  1  4 31  0  0  0]\n",
      " [ 0  0  4  1  2  9  0  3  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.10      0.06      0.07        17\n",
      "           5       0.21      0.14      0.17        29\n",
      "           6       0.09      0.62      0.16        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.38      0.16      0.22        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.09      0.11      0.07       160\n",
      "weighted avg       0.10      0.11      0.08       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "30 : epoch\n",
      "\n",
      "gloss : -106513336.8888889\n",
      "wd_loss : 623.2318793402778\n",
      "cls_loss : 2.083162307739258\n",
      "acc_t : 0.1307870414521959\n",
      "acc_s : 0.7453703880310059\n",
      "val_loss : 2.19627685546875\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  3  5  0  0  0]\n",
      " [ 0  0  1  0  2  7  0  0  0]\n",
      " [ 0  1  0  1  2 11  0  1  0]\n",
      " [ 0  1  0  1  3 11  0  0  1]\n",
      " [ 0  0  0  1  9 19  0  0  0]\n",
      " [ 0  0  0  2  1 10  0  3  0]\n",
      " [ 0  1  1  0 10 26  0  0  0]\n",
      " [ 0  0  4  1  6  7  0  1  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.17      0.06      0.09        17\n",
      "           5       0.24      0.31      0.27        29\n",
      "           6       0.10      0.62      0.17        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.20      0.05      0.08        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.08      0.12      0.07       160\n",
      "weighted avg       0.10      0.13      0.09       160\n",
      "\n",
      "best_val_loss : 2.1956127166748045, epoch : 14\n",
      "\n",
      "31 : epoch\n",
      "\n",
      "gloss : -129257187.55555555\n",
      "wd_loss : 672.8234592013889\n",
      "cls_loss : 2.0838197072347007\n",
      "acc_t : 0.1620370414521959\n",
      "acc_s : 0.7199073897467719\n",
      "val_loss : 2.195531225204468\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  2  5  0  0  0]\n",
      " [ 0  0  1  0  2  7  0  0  0]\n",
      " [ 0  0  0  1  4 10  0  1  0]\n",
      " [ 0  1  0  1  3 11  0  0  1]\n",
      " [ 0  0  0  1 11 17  0  0  0]\n",
      " [ 0  0  0  1  3  7  2  3  0]\n",
      " [ 0  0  1  0 15 20  0  2  0]\n",
      " [ 0  1  3  1  6  7  0  1  0]\n",
      " [ 0  0  0  0  1  6  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.17      0.06      0.09        17\n",
      "           5       0.23      0.38      0.29        29\n",
      "           6       0.08      0.44      0.13        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.14      0.05      0.08        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.07      0.10      0.07       160\n",
      "weighted avg       0.08      0.12      0.08       160\n",
      "\n",
      "best_val_loss : 2.195531225204468, epoch : 31\n",
      "\n",
      "32 : epoch\n",
      "\n",
      "gloss : -137173418.66666666\n",
      "wd_loss : 721.3827582465278\n",
      "cls_loss : 2.084516101413303\n",
      "acc_t : 0.13368055555555555\n",
      "acc_s : 0.7366898324754503\n",
      "val_loss : 2.1960543155670167\n",
      "confusion_matrix\n",
      "[[ 0  0  1  2  1  4  0  0  0]\n",
      " [ 0  1  1  0  2  6  0  0  0]\n",
      " [ 0  1  1  1  3  9  0  1  0]\n",
      " [ 0  1  0  2  2 11  0  0  1]\n",
      " [ 0  0  1  2  7 18  0  1  0]\n",
      " [ 0  0  0  2  1  9  2  2  0]\n",
      " [ 0  0  2  0  3 30  0  3  0]\n",
      " [ 0  1  3  0  2 11  1  1  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.25      0.10      0.14        10\n",
      "           3       0.11      0.06      0.08        16\n",
      "           4       0.22      0.12      0.15        17\n",
      "           5       0.33      0.24      0.28        29\n",
      "           6       0.09      0.56      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.12      0.05      0.07        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.13      0.13      0.10       160\n",
      "weighted avg       0.13      0.13      0.11       160\n",
      "\n",
      "best_val_loss : 2.195531225204468, epoch : 31\n",
      "\n",
      "33 : epoch\n",
      "\n",
      "gloss : -195120000.0\n",
      "wd_loss : 758.8046332465278\n",
      "cls_loss : 2.0785613589816623\n",
      "acc_t : 0.12442129188113743\n",
      "acc_s : 0.7824073897467719\n",
      "val_loss : 2.1947900295257567\n",
      "confusion_matrix\n",
      "[[ 0  0  2  1  1  3  0  1  0]\n",
      " [ 0  0  1  0  3  5  0  1  0]\n",
      " [ 0  1  1  1  4  8  0  1  0]\n",
      " [ 0  1  0  3  3  9  0  0  1]\n",
      " [ 0  0  1  2  7 17  1  1  0]\n",
      " [ 0  0  0  2  2  8  2  2  0]\n",
      " [ 0  0  1  1  9 24  1  2  0]\n",
      " [ 0  1  3  0  4  9  1  1  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.11      0.06      0.08        16\n",
      "           4       0.30      0.18      0.22        17\n",
      "           5       0.21      0.24      0.23        29\n",
      "           6       0.09      0.50      0.15        16\n",
      "           7       0.20      0.03      0.05        38\n",
      "           8       0.11      0.05      0.07        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.11      0.12      0.09       160\n",
      "weighted avg       0.15      0.13      0.11       160\n",
      "\n",
      "best_val_loss : 2.1947900295257567, epoch : 33\n",
      "\n",
      "34 : epoch\n",
      "\n",
      "gloss : -153096504.8888889\n",
      "wd_loss : 810.7345377604166\n",
      "cls_loss : 2.0759955512152777\n",
      "acc_t : 0.1388888888888889\n",
      "acc_s : 0.7349537213643392\n",
      "val_loss : 2.196651887893677\n",
      "confusion_matrix\n",
      "[[ 0  0  1  0  1  5  0  1  0]\n",
      " [ 0  0  0  0  3  7  0  0  0]\n",
      " [ 0  0  0  1  5  9  0  1  0]\n",
      " [ 0  1  0  1  4 10  0  0  1]\n",
      " [ 0  0  1  2  8 17  0  1  0]\n",
      " [ 0  0  1  1  4  8  1  1  0]\n",
      " [ 0  0  1  1 12 23  0  1  0]\n",
      " [ 0  0  2  0  5 10  1  1  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.17      0.06      0.09        17\n",
      "           5       0.19      0.28      0.23        29\n",
      "           6       0.08      0.50      0.14        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.17      0.05      0.08        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.07      0.10      0.06       160\n",
      "weighted avg       0.08      0.11      0.07       160\n",
      "\n",
      "best_val_loss : 2.1947900295257567, epoch : 33\n",
      "\n",
      "35 : epoch\n",
      "\n",
      "gloss : -183287950.2222222\n",
      "wd_loss : 883.3166232638889\n",
      "cls_loss : 2.0800079769558377\n",
      "acc_t : 0.12557870811886257\n",
      "acc_s : 0.7262731658087836\n",
      "val_loss : 2.1938851356506346\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  1  5  0  0  0]\n",
      " [ 0  0  0  0  3  7  0  0  0]\n",
      " [ 0  0  1  1  5  8  0  1  0]\n",
      " [ 0  0  1  4  4  6  0  1  1]\n",
      " [ 0  0  2  1  7 18  0  1  0]\n",
      " [ 0  0  1  0  4  8  1  2  0]\n",
      " [ 0  0  1  1 14 21  0  1  0]\n",
      " [ 0  0  2  1  5 10  1  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.11      0.06      0.08        16\n",
      "           4       0.44      0.24      0.31        17\n",
      "           5       0.16      0.24      0.19        29\n",
      "           6       0.09      0.50      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.09      0.12      0.08       160\n",
      "weighted avg       0.10      0.12      0.09       160\n",
      "\n",
      "best_val_loss : 2.1938851356506346, epoch : 35\n",
      "\n",
      "36 : epoch\n",
      "\n",
      "gloss : -330463971.5555556\n",
      "wd_loss : 922.5973307291666\n",
      "cls_loss : 2.0784998999701605\n",
      "acc_t : 0.13599537478552925\n",
      "acc_s : 0.7575231658087836\n",
      "val_loss : 2.194300317764282\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  1  4  0  1  0]\n",
      " [ 0  0  1  0  1  8  0  0  0]\n",
      " [ 0  0  1  2  4  9  0  0  0]\n",
      " [ 0  0  0  4  4  6  0  2  1]\n",
      " [ 1  0  1  4  5 17  0  1  0]\n",
      " [ 0  0  0  1  3  8  2  2  0]\n",
      " [ 0  1  1  2  6 26  1  1  0]\n",
      " [ 0  1  1  1  6 10  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.17      0.06      0.09        16\n",
      "           4       0.27      0.24      0.25        17\n",
      "           5       0.17      0.17      0.17        29\n",
      "           6       0.08      0.50      0.14        16\n",
      "           7       0.33      0.03      0.05        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.11      0.11      0.08       160\n",
      "weighted avg       0.16      0.12      0.09       160\n",
      "\n",
      "best_val_loss : 2.1938851356506346, epoch : 35\n",
      "\n",
      "37 : epoch\n",
      "\n",
      "gloss : -332804295.1111111\n",
      "wd_loss : 987.5784505208334\n",
      "cls_loss : 2.0707872178819446\n",
      "acc_t : 0.12152777777777778\n",
      "acc_s : 0.7615740564134386\n",
      "val_loss : 2.19648756980896\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  1  5  0  1  0]\n",
      " [ 0  0  1  0  1  8  0  0  0]\n",
      " [ 0  0  0  2  2 11  1  0  0]\n",
      " [ 0  0  0  5  4  7  0  1  0]\n",
      " [ 0  0  1  1  5 21  0  1  0]\n",
      " [ 0  1  0  1  3  8  0  3  0]\n",
      " [ 0  1  2  1  7 25  1  1  0]\n",
      " [ 0  0  2  1  4 12  0  0  0]\n",
      " [ 0  0  0  0  0  7  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.42      0.29      0.34        17\n",
      "           5       0.19      0.17      0.18        29\n",
      "           6       0.08      0.50      0.13        16\n",
      "           7       0.50      0.03      0.05        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.13      0.11      0.08       160\n",
      "weighted avg       0.20      0.12      0.09       160\n",
      "\n",
      "best_val_loss : 2.1938851356506346, epoch : 35\n",
      "\n",
      "38 : epoch\n",
      "\n",
      "gloss : -359523413.3333333\n",
      "wd_loss : 1039.821506076389\n",
      "cls_loss : 2.074437459309896\n",
      "acc_t : 0.11458333333333333\n",
      "acc_s : 0.7459490564134386\n",
      "val_loss : 2.193524122238159\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  2  4  0  1  0]\n",
      " [ 0  0  0  0  2  7  1  0  0]\n",
      " [ 0  0  0  1  3 10  2  0  0]\n",
      " [ 0  1  0  2  4  8  1  1  0]\n",
      " [ 0  0  1  0  8 19  0  1  0]\n",
      " [ 0  0  1  0  4  9  0  2  0]\n",
      " [ 0  0  3  1 15 18  0  1  0]\n",
      " [ 0  0  2  1  7  9  0  0  0]\n",
      " [ 0  0  0  0  1  5  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.12      0.17        17\n",
      "           5       0.17      0.28      0.21        29\n",
      "           6       0.10      0.56      0.17        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.07      0.11      0.06       160\n",
      "weighted avg       0.08      0.12      0.07       160\n",
      "\n",
      "best_val_loss : 2.193524122238159, epoch : 38\n",
      "\n",
      "39 : epoch\n",
      "\n",
      "gloss : -732330439.1111112\n",
      "wd_loss : 1091.3412543402778\n",
      "cls_loss : 2.0743946499294705\n",
      "acc_t : 0.1134259303410848\n",
      "acc_s : 0.768518500857883\n",
      "val_loss : 2.194810914993286\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  3  3  1  1  0]\n",
      " [ 0  0  0  0  3  7  0  0  0]\n",
      " [ 0  0  0  1  4  9  2  0  0]\n",
      " [ 0  0  0  3  4  8  1  1  0]\n",
      " [ 0  0  2  2  8 15  0  2  0]\n",
      " [ 0  0  1  1  3  9  1  1  0]\n",
      " [ 0  0  3  3 17 12  2  1  0]\n",
      " [ 0  1  2  1  6  9  0  0  0]\n",
      " [ 0  0  0  1  1  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.25      0.18      0.21        17\n",
      "           5       0.16      0.28      0.21        29\n",
      "           6       0.12      0.56      0.19        16\n",
      "           7       0.29      0.05      0.09        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.09      0.12      0.08       160\n",
      "weighted avg       0.14      0.14      0.10       160\n",
      "\n",
      "best_val_loss : 2.193524122238159, epoch : 38\n",
      "\n",
      "40 : epoch\n",
      "\n",
      "gloss : -544903111.1111112\n",
      "wd_loss : 1160.0633680555557\n",
      "cls_loss : 2.0680211385091147\n",
      "acc_t : 0.10532407628165351\n",
      "acc_s : 0.7771990564134386\n",
      "val_loss : 2.19707612991333\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  5  1  1  0]\n",
      " [ 0  1  0  0  2  7  0  0  0]\n",
      " [ 0  0  0  2  1  9  3  1  0]\n",
      " [ 0  0  0  5  3  9  0  0  0]\n",
      " [ 0  0  2  4  6 15  0  2  0]\n",
      " [ 0  1  0  3  1  8  1  2  0]\n",
      " [ 0  0  1  5  8 23  1  0  0]\n",
      " [ 0  1  1  1  5 11  0  0  0]\n",
      " [ 0  0  0  2  1  4  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.33      0.10      0.15        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.23      0.29      0.26        17\n",
      "           5       0.21      0.21      0.21        29\n",
      "           6       0.09      0.50      0.15        16\n",
      "           7       0.17      0.03      0.05        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.11      0.13      0.09       160\n",
      "weighted avg       0.13      0.13      0.10       160\n",
      "\n",
      "best_val_loss : 2.193524122238159, epoch : 38\n",
      "\n",
      "41 : epoch\n",
      "\n",
      "gloss : -821005937.7777778\n",
      "wd_loss : 1259.922634548611\n",
      "cls_loss : 2.0647012922498913\n",
      "acc_t : 0.10532407628165351\n",
      "acc_s : 0.7546296119689941\n",
      "val_loss : 2.1960198402404787\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  5  1  1  0]\n",
      " [ 0  1  0  0  3  6  0  0  0]\n",
      " [ 0  0  0  2  3  9  2  0  0]\n",
      " [ 0  0  0  4  5  7  1  0  0]\n",
      " [ 0  0  2  2  7 15  1  2  0]\n",
      " [ 0  1  0  0  2 10  1  2  0]\n",
      " [ 0  0  3  2 10 21  2  0  0]\n",
      " [ 0  1  2  0  5 10  1  0  0]\n",
      " [ 0  0  0  2  1  4  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.33      0.10      0.15        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.33      0.24      0.28        17\n",
      "           5       0.19      0.24      0.21        29\n",
      "           6       0.11      0.62      0.19        16\n",
      "           7       0.22      0.05      0.09        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.15       160\n",
      "   macro avg       0.13      0.14      0.10       160\n",
      "weighted avg       0.15      0.15      0.12       160\n",
      "\n",
      "best_val_loss : 2.193524122238159, epoch : 38\n",
      "\n",
      "42 : epoch\n",
      "\n",
      "gloss : -980939434.6666666\n",
      "wd_loss : 1334.4766710069443\n",
      "cls_loss : 2.0655606587727866\n",
      "acc_t : 0.13541666666666666\n",
      "acc_s : 0.765625\n",
      "val_loss : 2.195574426651001\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  2  5  1  0  0]\n",
      " [ 0  0  0  0  4  6  0  0  0]\n",
      " [ 0  0  1  2  4  7  1  1  0]\n",
      " [ 0  0  0  3  5  7  1  1  0]\n",
      " [ 0  0  2  0 10 15  1  1  0]\n",
      " [ 0  0  2  0  2 10  1  1  0]\n",
      " [ 0  0  3  1 19 14  1  0  0]\n",
      " [ 0  0  1  0  8  9  1  0  0]\n",
      " [ 0  1  0  1  2  3  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.11      0.06      0.08        16\n",
      "           4       0.43      0.18      0.25        17\n",
      "           5       0.18      0.34      0.24        29\n",
      "           6       0.13      0.62      0.22        16\n",
      "           7       0.14      0.03      0.04        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.16       160\n",
      "   macro avg       0.11      0.14      0.09       160\n",
      "weighted avg       0.14      0.16      0.11       160\n",
      "\n",
      "best_val_loss : 2.193524122238159, epoch : 38\n",
      "\n",
      "43 : epoch\n",
      "\n",
      "gloss : -1277960760.8888888\n",
      "wd_loss : 1433.02734375\n",
      "cls_loss : 2.064444647894965\n",
      "acc_t : 0.12268518077002631\n",
      "acc_s : 0.7702546119689941\n",
      "val_loss : 2.197052574157715\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  2  5  1  0  0]\n",
      " [ 0  0  0  0  3  6  0  1  0]\n",
      " [ 0  0  0  4  2  8  2  0  0]\n",
      " [ 0  0  0  3  4  8  1  1  0]\n",
      " [ 0  0  1  1  9 14  2  2  0]\n",
      " [ 0  0  2  1  1  9  1  2  0]\n",
      " [ 0  0  3  1  7 23  3  1  0]\n",
      " [ 0  0  1  0  5 12  1  0  0]\n",
      " [ 0  1  0  1  0  5  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.27      0.18      0.21        17\n",
      "           5       0.27      0.31      0.29        29\n",
      "           6       0.10      0.56      0.17        16\n",
      "           7       0.27      0.08      0.12        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.15       160\n",
      "   macro avg       0.10      0.13      0.09       160\n",
      "weighted avg       0.15      0.15      0.12       160\n",
      "\n",
      "best_val_loss : 2.193524122238159, epoch : 38\n",
      "\n",
      "44 : epoch\n",
      "\n",
      "gloss : -1601785628.4444444\n",
      "wd_loss : 1499.5806206597222\n",
      "cls_loss : 2.059096654256185\n",
      "acc_t : 0.11574073632558186\n",
      "acc_s : 0.8026620546976725\n",
      "val_loss : 2.193970727920532\n",
      "confusion_matrix\n",
      "[[ 0  0  0  2  2  3  1  0  0]\n",
      " [ 0  1  0  0  1  4  1  3  0]\n",
      " [ 0  0  0  5  2  7  1  1  0]\n",
      " [ 0  0  0  6  4  6  1  0  0]\n",
      " [ 0  0  2  8  6  9  3  1  0]\n",
      " [ 0  0  1  3  1  7  1  3  0]\n",
      " [ 0  0  2  5  8 20  3  0  0]\n",
      " [ 0  0  1  2  6  9  1  0  0]\n",
      " [ 0  0  0  3  0  4  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       1.00      0.10      0.18        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.18      0.35      0.24        17\n",
      "           5       0.20      0.21      0.20        29\n",
      "           6       0.10      0.44      0.16        16\n",
      "           7       0.25      0.08      0.12        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.19      0.13      0.10       160\n",
      "weighted avg       0.19      0.14      0.12       160\n",
      "\n",
      "best_val_loss : 2.193524122238159, epoch : 38\n",
      "\n",
      "45 : epoch\n",
      "\n",
      "gloss : -1325207096.8888888\n",
      "wd_loss : 1574.5091145833333\n",
      "cls_loss : 2.056321461995443\n",
      "acc_t : 0.11631944444444445\n",
      "acc_s : 0.828125\n",
      "val_loss : 2.190041971206665\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  4  1  1  0  0]\n",
      " [ 0  1  0  0  2  5  1  1  0]\n",
      " [ 0  0  1  2  3  8  2  0  0]\n",
      " [ 0  0  1  4  5  6  1  0  0]\n",
      " [ 0  0  2  3 13  9  2  0  0]\n",
      " [ 0  0  1  0  1 10  2  2  0]\n",
      " [ 0  0  3  1 19 12  3  0  0]\n",
      " [ 0  1  1  0 10  6  1  0  0]\n",
      " [ 0  0  2  2  2  1  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.50      0.10      0.17        10\n",
      "           3       0.08      0.06      0.07        16\n",
      "           4       0.31      0.24      0.27        17\n",
      "           5       0.22      0.45      0.30        29\n",
      "           6       0.17      0.62      0.27        16\n",
      "           7       0.23      0.08      0.12        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.20       160\n",
      "   macro avg       0.17      0.17      0.13       160\n",
      "weighted avg       0.18      0.20      0.15       160\n",
      "\n",
      "best_val_loss : 2.190041971206665, epoch : 45\n",
      "\n",
      "46 : epoch\n",
      "\n",
      "gloss : -1766124088.8888888\n",
      "wd_loss : 1616.631618923611\n",
      "cls_loss : 2.063103357950846\n",
      "acc_t : 0.13194444444444445\n",
      "acc_s : 0.7731481658087836\n",
      "val_loss : 2.193391942977905\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  2  4  1  0  0]\n",
      " [ 0  0  0  1  2  5  0  2  0]\n",
      " [ 0  0  0  4  3  9  0  0  0]\n",
      " [ 0  0  1  2  3  8  1  2  0]\n",
      " [ 0  0  1  4  9 15  0  0  0]\n",
      " [ 0  0  0  0  1 10  1  4  0]\n",
      " [ 0  0  1  1  6 29  1  0  0]\n",
      " [ 0  0  1  0  5 13  0  0  0]\n",
      " [ 0  0  0  1  2  4  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.14      0.12      0.13        17\n",
      "           5       0.27      0.31      0.29        29\n",
      "           6       0.10      0.62      0.18        16\n",
      "           7       0.25      0.03      0.05        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.09      0.12      0.07       160\n",
      "weighted avg       0.13      0.14      0.10       160\n",
      "\n",
      "best_val_loss : 2.190041971206665, epoch : 45\n",
      "\n",
      "47 : epoch\n",
      "\n",
      "gloss : -2481434624.0\n",
      "wd_loss : 1703.3624131944443\n",
      "cls_loss : 2.061931398179796\n",
      "acc_t : 0.12210648589664036\n",
      "acc_s : 0.7395833333333334\n",
      "val_loss : 2.191511058807373\n",
      "confusion_matrix\n",
      "[[ 0  1  0  1  1  5  0  0  0]\n",
      " [ 0  0  0  1  2  5  0  2  0]\n",
      " [ 0  0  0  5  3  6  0  2  0]\n",
      " [ 0  0  1  2  3  8  1  2  0]\n",
      " [ 0  0  1  3  8 16  1  0  0]\n",
      " [ 0  1  0  3  1  7  1  3  0]\n",
      " [ 0  0  1  1  5 28  1  2  0]\n",
      " [ 0  0  0  2  6 11  0  0  0]\n",
      " [ 0  0  0  2  1  4  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.10      0.12      0.11        17\n",
      "           5       0.27      0.28      0.27        29\n",
      "           6       0.08      0.44      0.13        16\n",
      "           7       0.25      0.03      0.05        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.08      0.10      0.06       160\n",
      "weighted avg       0.13      0.11      0.09       160\n",
      "\n",
      "best_val_loss : 2.190041971206665, epoch : 45\n",
      "\n",
      "48 : epoch\n",
      "\n",
      "gloss : -3231317788.4444447\n",
      "wd_loss : 1773.8057725694443\n",
      "cls_loss : 2.055873235066732\n",
      "acc_t : 0.11863426367441814\n",
      "acc_s : 0.8136573897467719\n",
      "val_loss : 2.185325860977173\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  4  2  0  0  0]\n",
      " [ 0  0  0  2  3  2  1  2  0]\n",
      " [ 0  1  0  5  2  6  1  1  0]\n",
      " [ 0  0  1  4  7  4  1  0  0]\n",
      " [ 0  0  1  4 17  6  1  0  0]\n",
      " [ 0  0  1  4  1  7  1  2  0]\n",
      " [ 0  1  2  2 21  7  3  2  0]\n",
      " [ 0  0  2  1  7  6  2  1  0]\n",
      " [ 0  0  2  1  3  1  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.17      0.24      0.20        17\n",
      "           5       0.26      0.59      0.36        29\n",
      "           6       0.17      0.44      0.25        16\n",
      "           7       0.30      0.08      0.12        38\n",
      "           8       0.12      0.05      0.07        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.20       160\n",
      "   macro avg       0.11      0.15      0.11       160\n",
      "weighted avg       0.17      0.20      0.15       160\n",
      "\n",
      "best_val_loss : 2.185325860977173, epoch : 48\n",
      "\n",
      "49 : epoch\n",
      "\n",
      "gloss : -2332327936.0\n",
      "wd_loss : 1886.4403211805557\n",
      "cls_loss : 2.054876539442274\n",
      "acc_t : 0.14293981922997367\n",
      "acc_s : 0.8119212786356608\n",
      "val_loss : 2.1851123332977296\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  4  3  0  1  0]\n",
      " [ 0  0  0  2  3  3  1  1  0]\n",
      " [ 0  0  0  5  2  6  2  1  0]\n",
      " [ 0  0  1  3  6  5  1  1  0]\n",
      " [ 0  0  4  2 15  7  1  0  0]\n",
      " [ 0  0  1  3  1  7  1  3  0]\n",
      " [ 0  0  2  1 24  9  1  1  0]\n",
      " [ 0  0  2  0  7  7  2  1  0]\n",
      " [ 0  0  1  1  4  1  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.18      0.18      0.18        17\n",
      "           5       0.23      0.52      0.32        29\n",
      "           6       0.15      0.44      0.22        16\n",
      "           7       0.11      0.03      0.04        38\n",
      "           8       0.11      0.05      0.07        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.17       160\n",
      "   macro avg       0.09      0.13      0.09       160\n",
      "weighted avg       0.11      0.17      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "50 : epoch\n",
      "\n",
      "gloss : -3389349660.4444447\n",
      "wd_loss : 2020.7261284722222\n",
      "cls_loss : 2.053024927775065\n",
      "acc_t : 0.12037037478552924\n",
      "acc_s : 0.7638888888888888\n",
      "val_loss : 2.1887065887451174\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  6  0  1  0]\n",
      " [ 0  0  0  2  1  5  1  1  0]\n",
      " [ 0  0  0  5  4  6  1  0  0]\n",
      " [ 0  0  0  3  3  9  0  2  0]\n",
      " [ 0  0  2  3  7 16  0  1  0]\n",
      " [ 0  0  1  2  1  8  1  3  0]\n",
      " [ 0  0  1  0  6 28  1  2  0]\n",
      " [ 0  0  0  0  5 12  1  1  0]\n",
      " [ 0  0  0  1  3  3  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.19      0.18      0.18        17\n",
      "           5       0.23      0.24      0.23        29\n",
      "           6       0.09      0.50      0.15        16\n",
      "           7       0.20      0.03      0.05        38\n",
      "           8       0.09      0.05      0.07        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.09      0.11      0.08       160\n",
      "weighted avg       0.13      0.12      0.10       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "51 : epoch\n",
      "\n",
      "gloss : -1765020785.7777777\n",
      "wd_loss : 2087.0477430555557\n",
      "cls_loss : 2.052858140733507\n",
      "acc_t : 0.12731481922997367\n",
      "acc_s : 0.7459490564134386\n",
      "val_loss : 2.1882054805755615\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  1  5  0  2  0]\n",
      " [ 0  0  0  2  1  4  1  2  0]\n",
      " [ 0  0  0  5  4  5  1  1  0]\n",
      " [ 0  0  0  4  3  8  0  2  0]\n",
      " [ 0  0  0  4 10 14  0  1  0]\n",
      " [ 0  0  0  3  1  7  1  4  0]\n",
      " [ 0  0  1  3  8 21  1  4  0]\n",
      " [ 0  0  0  2  4  9  1  3  0]\n",
      " [ 0  0  0  2  2  3  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.16      0.24      0.19        17\n",
      "           5       0.29      0.34      0.32        29\n",
      "           6       0.09      0.44      0.15        16\n",
      "           7       0.20      0.03      0.05        38\n",
      "           8       0.16      0.16      0.16        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.16       160\n",
      "   macro avg       0.10      0.13      0.10       160\n",
      "weighted avg       0.15      0.16      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "52 : epoch\n",
      "\n",
      "gloss : -5123374193.777778\n",
      "wd_loss : 2211.8155381944443\n",
      "cls_loss : 2.048545413547092\n",
      "acc_t : 0.14351851410335964\n",
      "acc_s : 0.7974537213643392\n",
      "val_loss : 2.1890981674194334\n",
      "confusion_matrix\n",
      "[[ 0  1  0  1  1  5  0  0  0]\n",
      " [ 0  0  0  2  2  4  1  1  0]\n",
      " [ 0  0  0  4  4  5  2  1  0]\n",
      " [ 0  0  1  4  4  7  0  1  0]\n",
      " [ 0  0  1  3 13  8  2  2  0]\n",
      " [ 0  0  1  3  1  7  1  3  0]\n",
      " [ 0  1  1  4 12 16  1  3  0]\n",
      " [ 0  0  1  2  3  8  2  3  0]\n",
      " [ 0  0  1  1  2  2  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.17      0.24      0.20        17\n",
      "           5       0.31      0.45      0.37        29\n",
      "           6       0.11      0.44      0.18        16\n",
      "           7       0.10      0.03      0.04        38\n",
      "           8       0.21      0.16      0.18        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.17       160\n",
      "   macro avg       0.10      0.15      0.11       160\n",
      "weighted avg       0.13      0.17      0.14       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "53 : epoch\n",
      "\n",
      "gloss : -4508469475.555555\n",
      "wd_loss : 2231.2400173611113\n",
      "cls_loss : 2.0540330674913196\n",
      "acc_t : 0.1365740696589152\n",
      "acc_s : 0.8049768341912164\n",
      "val_loss : 2.1955801486968993\n",
      "confusion_matrix\n",
      "[[ 0  1  0  0  2  5  0  0  0]\n",
      " [ 0  0  0  2  2  4  1  1  0]\n",
      " [ 0  0  0  4  5  5  1  1  0]\n",
      " [ 0  1  1  3  5  6  0  1  0]\n",
      " [ 0  0  2  3 12  8  3  1  0]\n",
      " [ 0  0  0  2  2  8  1  3  0]\n",
      " [ 0  1  1  1 16 15  1  3  0]\n",
      " [ 0  0  1  0  4  9  2  3  0]\n",
      " [ 0  0  1  0  3  2  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.20      0.18      0.19        17\n",
      "           5       0.24      0.41      0.30        29\n",
      "           6       0.13      0.50      0.21        16\n",
      "           7       0.10      0.03      0.04        38\n",
      "           8       0.23      0.16      0.19        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.17       160\n",
      "   macro avg       0.10      0.14      0.10       160\n",
      "weighted avg       0.13      0.17      0.13       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "54 : epoch\n",
      "\n",
      "gloss : -6868349838.222222\n",
      "wd_loss : 2371.658637152778\n",
      "cls_loss : 2.048150804307726\n",
      "acc_t : 0.1371527777777778\n",
      "acc_s : 0.7592592769198947\n",
      "val_loss : 2.2000551223754883\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  1  6  0  0  0]\n",
      " [ 0  0  0  2  3  3  0  2  0]\n",
      " [ 0  0  0  4  5  5  1  1  0]\n",
      " [ 0  0  0  3  6  6  0  2  0]\n",
      " [ 0  0  0  4 10 10  2  3  0]\n",
      " [ 0  0  0  2  2  8  2  2  0]\n",
      " [ 0  0  1  4 10 20  0  3  0]\n",
      " [ 0  0  1  1  4  8  2  3  0]\n",
      " [ 0  0  1  0  3  1  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.14      0.18      0.16        17\n",
      "           5       0.23      0.34      0.27        29\n",
      "           6       0.12      0.50      0.19        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.18      0.16      0.17        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.15       160\n",
      "   macro avg       0.07      0.13      0.09       160\n",
      "weighted avg       0.09      0.15      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "55 : epoch\n",
      "\n",
      "gloss : -5012390798.222222\n",
      "wd_loss : 2387.142144097222\n",
      "cls_loss : 2.0502906375461154\n",
      "acc_t : 0.13252315256330702\n",
      "acc_s : 0.7453703880310059\n",
      "val_loss : 2.204032278060913\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  2  5  0  0  0]\n",
      " [ 0  0  0  2  4  2  0  2  0]\n",
      " [ 0  0  1  3  4  6  1  1  0]\n",
      " [ 0  0  1  3  6  5  1  1  0]\n",
      " [ 0  0  0  5  9 11  2  2  0]\n",
      " [ 0  0  0  0  2  7  2  5  0]\n",
      " [ 0  0  0  3 11 20  0  4  0]\n",
      " [ 0  0  1  0  4  8  3  3  0]\n",
      " [ 0  0  0  0  1  3  2  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.33      0.06      0.11        16\n",
      "           4       0.18      0.18      0.18        17\n",
      "           5       0.21      0.31      0.25        29\n",
      "           6       0.10      0.44      0.17        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.16      0.16      0.16        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.11      0.13      0.10       160\n",
      "weighted avg       0.12      0.14      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "56 : epoch\n",
      "\n",
      "gloss : -8306597888.0\n",
      "wd_loss : 2590.136284722222\n",
      "cls_loss : 2.0474026997884116\n",
      "acc_t : 0.14930555555555555\n",
      "acc_s : 0.7847222222222222\n",
      "val_loss : 2.2056440830230715\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  3  4  0  0  0]\n",
      " [ 0  0  0  2  3  4  0  1  0]\n",
      " [ 0  0  1  3  4  6  1  1  0]\n",
      " [ 0  0  2  3  5  4  2  1  0]\n",
      " [ 0  0  1  5  9 11  2  1  0]\n",
      " [ 0  0  0  0  3  6  2  5  0]\n",
      " [ 0  0  1  2 15 16  0  4  0]\n",
      " [ 0  0  1  0  5  9  3  1  0]\n",
      " [ 0  0  1  2  2  1  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.14      0.06      0.09        16\n",
      "           4       0.17      0.18      0.17        17\n",
      "           5       0.18      0.31      0.23        29\n",
      "           6       0.10      0.38      0.16        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.07      0.05      0.06        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.07      0.11      0.08       160\n",
      "weighted avg       0.08      0.12      0.09       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "57 : epoch\n",
      "\n",
      "gloss : -7103505749.333333\n",
      "wd_loss : 2642.581597222222\n",
      "cls_loss : 2.0502332051595054\n",
      "acc_t : 0.1388888888888889\n",
      "acc_s : 0.8032407230801053\n",
      "val_loss : 2.202709913253784\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  1  5  0  0  0]\n",
      " [ 0  0  0  2  4  3  0  1  0]\n",
      " [ 0  0  1  4  6  3  1  1  0]\n",
      " [ 0  0  2  4  4  5  2  0  0]\n",
      " [ 0  0  2  6  6 12  3  0  0]\n",
      " [ 0  0  0  2  3  6  1  4  0]\n",
      " [ 0  0  2  3 10 18  0  5  0]\n",
      " [ 0  0  1  0  5  8  3  2  0]\n",
      " [ 0  0  1  2  1  2  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.10      0.06      0.08        16\n",
      "           4       0.17      0.24      0.20        17\n",
      "           5       0.15      0.21      0.17        29\n",
      "           6       0.10      0.38      0.15        16\n",
      "           7       0.00      0.00      0.00        38\n",
      "           8       0.15      0.11      0.12        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.07      0.11      0.08       160\n",
      "weighted avg       0.08      0.12      0.09       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "58 : epoch\n",
      "\n",
      "gloss : -6655304135.111111\n",
      "wd_loss : 2846.9989149305557\n",
      "cls_loss : 2.0444359249538846\n",
      "acc_t : 0.1388888888888889\n",
      "acc_s : 0.8142361111111112\n",
      "val_loss : 2.202182388305664\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  1  5  1  0  0]\n",
      " [ 0  0  0  2  3  4  0  1  0]\n",
      " [ 0  0  0  5  5  3  2  1  0]\n",
      " [ 0  0  2  4  2  7  2  0  0]\n",
      " [ 0  0  1  6  5 14  3  0  0]\n",
      " [ 0  0  0  2  3  7  1  3  0]\n",
      " [ 0  0  0  3  9 22  1  3  0]\n",
      " [ 0  0  0  1  5  9  4  0  0]\n",
      " [ 0  0  0  1  0  4  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.16      0.24      0.19        17\n",
      "           5       0.15      0.17      0.16        29\n",
      "           6       0.09      0.44      0.15        16\n",
      "           7       0.07      0.03      0.04        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.05      0.10      0.06       160\n",
      "weighted avg       0.07      0.11      0.07       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "59 : epoch\n",
      "\n",
      "gloss : -8804632348.444445\n",
      "wd_loss : 2939.0399305555557\n",
      "cls_loss : 2.0406032138400607\n",
      "acc_t : 0.13425926367441812\n",
      "acc_s : 0.8078703880310059\n",
      "val_loss : 2.1974020481109617\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  2  5  0  0  0]\n",
      " [ 0  1  0  1  4  3  0  1  0]\n",
      " [ 0  0  0  5  4  3  3  1  0]\n",
      " [ 0  1  1  4  2  6  2  1  0]\n",
      " [ 0  0  1  5  5 14  4  0  0]\n",
      " [ 0  0  0  3  4  7  0  2  0]\n",
      " [ 0  0  0  2 11 20  3  2  0]\n",
      " [ 0  0  0  2  6  7  4  0  0]\n",
      " [ 0  0  0  1  0  4  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.50      0.10      0.17        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.17      0.24      0.20        17\n",
      "           5       0.13      0.17      0.15        29\n",
      "           6       0.10      0.44      0.16        16\n",
      "           7       0.18      0.08      0.11        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.12      0.11      0.09       160\n",
      "weighted avg       0.12      0.12      0.10       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "60 : epoch\n",
      "\n",
      "gloss : -8048646371.555555\n",
      "wd_loss : 3073.5603298611113\n",
      "cls_loss : 2.0376510620117188\n",
      "acc_t : 0.14988426367441812\n",
      "acc_s : 0.8125\n",
      "val_loss : 2.1972657680511474\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  3  4  0  0  0]\n",
      " [ 0  0  0  1  5  2  0  2  0]\n",
      " [ 0  0  0  4  3  3  4  2  0]\n",
      " [ 0  0  1  4  7  2  1  2  0]\n",
      " [ 0  0  1  4  7  9  7  1  0]\n",
      " [ 0  0  0  1  6  7  0  2  0]\n",
      " [ 0  0  0  0 17 14  5  2  0]\n",
      " [ 0  0  0  2  7  6  4  0  0]\n",
      " [ 0  0  0  1  1  3  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.22      0.24      0.23        17\n",
      "           5       0.12      0.24      0.16        29\n",
      "           6       0.14      0.44      0.21        16\n",
      "           7       0.23      0.13      0.17        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.08      0.12      0.09       160\n",
      "weighted avg       0.11      0.14      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "61 : epoch\n",
      "\n",
      "gloss : -9788429653.333334\n",
      "wd_loss : 3104.382378472222\n",
      "cls_loss : 2.0436282687717013\n",
      "acc_t : 0.14178240299224854\n",
      "acc_s : 0.8125\n",
      "val_loss : 2.197340726852417\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  3  4  0  0  0]\n",
      " [ 0  0  0  0  6  2  0  2  0]\n",
      " [ 0  0  0  3  3  3  4  3  0]\n",
      " [ 0  0  2  3  6  3  1  2  0]\n",
      " [ 0  0  2  2  7 12  5  1  0]\n",
      " [ 0  0  0  1  5  7  0  3  0]\n",
      " [ 0  0  1  0 14 16  6  1  0]\n",
      " [ 0  0  1  0  7  8  3  0  0]\n",
      " [ 0  0  1  1  1  4  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.27      0.18      0.21        17\n",
      "           5       0.13      0.24      0.17        29\n",
      "           6       0.12      0.44      0.19        16\n",
      "           7       0.32      0.16      0.21        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.09      0.11      0.09       160\n",
      "weighted avg       0.14      0.14      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "62 : epoch\n",
      "\n",
      "gloss : -13832406812.444445\n",
      "wd_loss : 3309.7278645833335\n",
      "cls_loss : 2.0421371459960938\n",
      "acc_t : 0.14004629188113743\n",
      "acc_s : 0.8043981658087836\n",
      "val_loss : 2.194599962234497\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  3  4  0  0  0]\n",
      " [ 0  0  0  1  4  3  0  2  0]\n",
      " [ 0  0  0  6  3  3  3  1  0]\n",
      " [ 0  0  2  3  6  3  1  2  0]\n",
      " [ 0  0  1  2  5 15  5  1  0]\n",
      " [ 0  0  0  1  5  7  0  3  0]\n",
      " [ 0  0  0  0 11 21  6  0  0]\n",
      " [ 0  0  1  1  7  9  1  0  0]\n",
      " [ 0  0  0  0  1  5  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.20      0.18      0.19        17\n",
      "           5       0.11      0.17      0.14        29\n",
      "           6       0.10      0.44      0.16        16\n",
      "           7       0.35      0.16      0.22        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.08      0.10      0.08       160\n",
      "weighted avg       0.14      0.13      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "63 : epoch\n",
      "\n",
      "gloss : -11726388337.777779\n",
      "wd_loss : 3384.7565104166665\n",
      "cls_loss : 2.037107891506619\n",
      "acc_t : 0.13368055555555555\n",
      "acc_s : 0.8038194444444444\n",
      "val_loss : 2.192672061920166\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  3  4  0  0  0]\n",
      " [ 0  0  0  2  3  3  0  2  0]\n",
      " [ 0  0  0  5  4  4  2  1  0]\n",
      " [ 0  0  0  2  4  7  1  3  0]\n",
      " [ 0  0  1  3  4 12  8  1  0]\n",
      " [ 0  0  0  3  5  6  0  2  0]\n",
      " [ 0  0  0  1 11 21  5  0  0]\n",
      " [ 0  0  1  2  6  9  1  0  0]\n",
      " [ 0  0  0  0  1  5  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.11      0.12      0.11        17\n",
      "           5       0.10      0.14      0.11        29\n",
      "           6       0.08      0.38      0.14        16\n",
      "           7       0.28      0.13      0.18        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.11       160\n",
      "   macro avg       0.06      0.08      0.06       160\n",
      "weighted avg       0.10      0.11      0.09       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "64 : epoch\n",
      "\n",
      "gloss : -17064744277.333334\n",
      "wd_loss : 3526.2361111111113\n",
      "cls_loss : 2.032310273912218\n",
      "acc_t : 0.13368055555555555\n",
      "acc_s : 0.8159722222222222\n",
      "val_loss : 2.1954660415649414\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  5  2  0  0  0]\n",
      " [ 0  0  0  2  3  3  0  2  0]\n",
      " [ 0  0  1  4  3  3  2  3  0]\n",
      " [ 0  0  1  2  6  4  1  3  0]\n",
      " [ 0  0  1  3  6 10  7  2  0]\n",
      " [ 0  0  0  6  5  4  0  1  0]\n",
      " [ 0  0  0  2 15 16  5  0  0]\n",
      " [ 0  0  1  1  9  6  1  1  0]\n",
      " [ 0  0  0  1  3  2  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.25      0.06      0.10        16\n",
      "           4       0.09      0.12      0.10        17\n",
      "           5       0.11      0.21      0.14        29\n",
      "           6       0.08      0.25      0.12        16\n",
      "           7       0.29      0.13      0.18        38\n",
      "           8       0.08      0.05      0.06        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.10      0.09      0.08       160\n",
      "weighted avg       0.14      0.12      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "65 : epoch\n",
      "\n",
      "gloss : -15480049208.88889\n",
      "wd_loss : 3701.9943576388887\n",
      "cls_loss : 2.034881167941623\n",
      "acc_t : 0.13425926367441812\n",
      "acc_s : 0.8246527777777778\n",
      "val_loss : 2.194496726989746\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  4  3  0  1  0]\n",
      " [ 0  0  0  0  3  4  0  3  0]\n",
      " [ 0  0  1  3  2  4  3  3  0]\n",
      " [ 0  0  2  2  5  5  1  2  0]\n",
      " [ 0  0  1  3  9 10  4  2  0]\n",
      " [ 0  0  1  3  4  6  1  1  0]\n",
      " [ 0  0  0  2 15 17  4  0  0]\n",
      " [ 0  0  1  0 10  7  1  0  0]\n",
      " [ 0  0  0  0  1  5  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.17      0.06      0.09        16\n",
      "           4       0.15      0.12      0.13        17\n",
      "           5       0.17      0.31      0.22        29\n",
      "           6       0.10      0.38      0.16        16\n",
      "           7       0.27      0.11      0.15        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.10      0.11      0.08       160\n",
      "weighted avg       0.14      0.14      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "66 : epoch\n",
      "\n",
      "gloss : -12314627640.88889\n",
      "wd_loss : 3841.073784722222\n",
      "cls_loss : 2.030878914727105\n",
      "acc_t : 0.13020833333333334\n",
      "acc_s : 0.8015046119689941\n",
      "val_loss : 2.1974980354309084\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  2  5  0  1  0]\n",
      " [ 0  0  0  0  2  4  1  3  0]\n",
      " [ 0  0  1  4  2  4  2  3  0]\n",
      " [ 0  0  2  2  4  6  1  2  0]\n",
      " [ 0  0  1  4  5 11  5  3  0]\n",
      " [ 0  0  0  2  3  7  2  2  0]\n",
      " [ 0  0  0  1 13 18  5  1  0]\n",
      " [ 0  0  1  0  7  9  1  1  0]\n",
      " [ 0  0  0  1  2  3  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.20      0.06      0.10        16\n",
      "           4       0.14      0.12      0.13        17\n",
      "           5       0.12      0.17      0.14        29\n",
      "           6       0.10      0.44      0.17        16\n",
      "           7       0.28      0.13      0.18        38\n",
      "           8       0.06      0.05      0.06        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.10      0.11      0.09       160\n",
      "weighted avg       0.14      0.13      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "67 : epoch\n",
      "\n",
      "gloss : -24791618901.333332\n",
      "wd_loss : 4104.368489583333\n",
      "cls_loss : 2.023168775770399\n",
      "acc_t : 0.12731481922997367\n",
      "acc_s : 0.8032407230801053\n",
      "val_loss : 2.199656105041504\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  2  5  0  1  0]\n",
      " [ 0  0  0  2  2  4  0  2  0]\n",
      " [ 0  1  2  3  2  4  2  2  0]\n",
      " [ 0  0  1  2  4  6  2  2  0]\n",
      " [ 0  0  1  6  5 12  3  2  0]\n",
      " [ 0  1  1  5  1  5  2  1  0]\n",
      " [ 0  0  1  2  7 21  6  1  0]\n",
      " [ 0  0  1  1  8  8  1  0  0]\n",
      " [ 0  1  0  2  1  2  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.29      0.12      0.17        16\n",
      "           4       0.09      0.12      0.10        17\n",
      "           5       0.16      0.17      0.16        29\n",
      "           6       0.07      0.31      0.12        16\n",
      "           7       0.35      0.16      0.22        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.11      0.10      0.09       160\n",
      "weighted avg       0.16      0.12      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "68 : epoch\n",
      "\n",
      "gloss : -28161241543.11111\n",
      "wd_loss : 4082.8420138888887\n",
      "cls_loss : 2.0270993974473743\n",
      "acc_t : 0.13252315256330702\n",
      "acc_s : 0.7771990564134386\n",
      "val_loss : 2.1990407943725585\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  3  4  0  1  0]\n",
      " [ 0  0  0  2  3  3  0  2  0]\n",
      " [ 0  1  2  2  3  4  3  1  0]\n",
      " [ 0  0  1  1  4  7  1  3  0]\n",
      " [ 0  0  1  5  5 11  4  3  0]\n",
      " [ 0  1  0  5  1  7  1  1  0]\n",
      " [ 0  0  1  2  8 20  5  2  0]\n",
      " [ 0  0  1  0  8  9  1  0  0]\n",
      " [ 0  1  0  2  1  2  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.33      0.12      0.18        16\n",
      "           4       0.05      0.06      0.06        17\n",
      "           5       0.14      0.17      0.15        29\n",
      "           6       0.10      0.44      0.17        16\n",
      "           7       0.31      0.13      0.19        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.10      0.10      0.08       160\n",
      "weighted avg       0.15      0.12      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "69 : epoch\n",
      "\n",
      "gloss : -23326355911.11111\n",
      "wd_loss : 4353.995225694444\n",
      "cls_loss : 2.025604248046875\n",
      "acc_t : 0.13310184743669298\n",
      "acc_s : 0.7731481658087836\n",
      "val_loss : 2.1995325565338133\n",
      "confusion_matrix\n",
      "[[ 0  0  0  0  4  1  0  3  0]\n",
      " [ 0  0  0  2  3  2  1  2  0]\n",
      " [ 0  0  1  2  3  5  2  3  0]\n",
      " [ 0  0  1  1  4  6  2  3  0]\n",
      " [ 0  0  1  3  6 12  3  4  0]\n",
      " [ 0  0  0  5  1  6  1  3  0]\n",
      " [ 0  0  1  3 15 13  4  2  0]\n",
      " [ 0  0  1  1  8  6  2  1  0]\n",
      " [ 0  1  0  1  1  2  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.20      0.06      0.10        16\n",
      "           4       0.06      0.06      0.06        17\n",
      "           5       0.13      0.21      0.16        29\n",
      "           6       0.11      0.38      0.17        16\n",
      "           7       0.25      0.11      0.15        38\n",
      "           8       0.05      0.05      0.05        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.09      0.10      0.08       160\n",
      "weighted avg       0.13      0.12      0.10       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "70 : epoch\n",
      "\n",
      "gloss : -44582458709.333336\n",
      "wd_loss : 4484.614583333333\n",
      "cls_loss : 2.023189968532986\n",
      "acc_t : 0.1371527777777778\n",
      "acc_s : 0.7934027777777778\n",
      "val_loss : 2.1983526706695558\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  2  2  0  3  0]\n",
      " [ 0  0  0  2  3  1  1  3  0]\n",
      " [ 0  0  0  3  3  5  2  3  0]\n",
      " [ 0  0  2  1  3  6  2  3  0]\n",
      " [ 0  0  1  5  7  6  2  8  0]\n",
      " [ 0  0  0  6  1  7  1  1  0]\n",
      " [ 0  0  1  7 17  6  4  3  0]\n",
      " [ 0  0  1  2  8  5  2  1  0]\n",
      " [ 0  1  0  1  1  1  1  2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.04      0.06      0.04        17\n",
      "           5       0.16      0.24      0.19        29\n",
      "           6       0.18      0.44      0.25        16\n",
      "           7       0.27      0.11      0.15        38\n",
      "           8       0.04      0.05      0.04        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.07      0.10      0.08       160\n",
      "weighted avg       0.12      0.12      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "71 : epoch\n",
      "\n",
      "gloss : -32416821703.11111\n",
      "wd_loss : 4565.482204861111\n",
      "cls_loss : 2.0198112063937717\n",
      "acc_t : 0.1440972222222222\n",
      "acc_s : 0.8107638888888888\n",
      "val_loss : 2.1976128101348875\n",
      "confusion_matrix\n",
      "[[ 0  0  1  0  0  2  2  3  0]\n",
      " [ 0  0  0  2  3  1  1  3  0]\n",
      " [ 0  0  0  5  2  5  2  2  0]\n",
      " [ 0  1  1  4  2  5  2  2  0]\n",
      " [ 0  0  1 11  6  4  2  5  0]\n",
      " [ 0  0  0  7  0  7  1  1  0]\n",
      " [ 0  0  1 13 12  6  3  3  0]\n",
      " [ 0  0  1  4  3  6  4  1  0]\n",
      " [ 0  1  0  3  1  0  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.08      0.24      0.12        17\n",
      "           5       0.21      0.21      0.21        29\n",
      "           6       0.19      0.44      0.27        16\n",
      "           7       0.17      0.08      0.11        38\n",
      "           8       0.05      0.05      0.05        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.08      0.11      0.08       160\n",
      "weighted avg       0.11      0.13      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "72 : epoch\n",
      "\n",
      "gloss : -44906344903.111115\n",
      "wd_loss : 4840.666666666667\n",
      "cls_loss : 2.0144176483154297\n",
      "acc_t : 0.14178240299224854\n",
      "acc_s : 0.7951388888888888\n",
      "val_loss : 2.1931700706481934\n",
      "confusion_matrix\n",
      "[[ 0  0  1  0  0  3  2  2  0]\n",
      " [ 0  0  0  2  3  1  1  3  0]\n",
      " [ 0  0  0  4  3  4  2  3  0]\n",
      " [ 0  0  1  2  3  5  3  3  0]\n",
      " [ 0  0  1 11  7  7  1  2  0]\n",
      " [ 0  0  0  7  1  6  1  1  0]\n",
      " [ 0  0  0  7 13 10  6  2  0]\n",
      " [ 0  0  0  4  4  9  1  1  0]\n",
      " [ 0  1  0  1  1  1  2  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.05      0.12      0.07        17\n",
      "           5       0.20      0.24      0.22        29\n",
      "           6       0.13      0.38      0.19        16\n",
      "           7       0.32      0.16      0.21        38\n",
      "           8       0.06      0.05      0.05        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.08      0.10      0.08       160\n",
      "weighted avg       0.14      0.14      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "73 : epoch\n",
      "\n",
      "gloss : -32283015395.555557\n",
      "wd_loss : 4956.942708333333\n",
      "cls_loss : 2.0178926255967884\n",
      "acc_t : 0.1527777777777778\n",
      "acc_s : 0.7951388888888888\n",
      "val_loss : 2.1928032875061034\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  1  3  0  2  0]\n",
      " [ 0  0  0  1  5  1  0  3  0]\n",
      " [ 0  0  0  3  4  6  2  1  0]\n",
      " [ 0  0  1  2  3  7  3  1  0]\n",
      " [ 0  1  1  6 10  5  2  4  0]\n",
      " [ 0  0  0  4  1  8  1  2  0]\n",
      " [ 0  0  0  2 21 10  4  1  0]\n",
      " [ 0  0  1  1  5 11  1  0  0]\n",
      " [ 0  0  0  1  2  2  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.10      0.12      0.11        17\n",
      "           5       0.19      0.34      0.25        29\n",
      "           6       0.15      0.50      0.23        16\n",
      "           7       0.29      0.11      0.15        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.15       160\n",
      "   macro avg       0.08      0.12      0.08       160\n",
      "weighted avg       0.13      0.15      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "74 : epoch\n",
      "\n",
      "gloss : -44615044664.888885\n",
      "wd_loss : 5156.230034722223\n",
      "cls_loss : 2.0160899692111545\n",
      "acc_t : 0.16145833333333334\n",
      "acc_s : 0.7575231658087836\n",
      "val_loss : 2.1954887390136717\n",
      "confusion_matrix\n",
      "[[ 0  0  1  0  0  4  1  2  0]\n",
      " [ 0  0  0  1  3  2  1  3  0]\n",
      " [ 0  0  0  4  5  4  2  1  0]\n",
      " [ 0  0  1  4  3  6  1  2  0]\n",
      " [ 0  1  1  5  8  8  1  5  0]\n",
      " [ 0  0  0  4  0  8  0  4  0]\n",
      " [ 0  0  0  3 14 15  3  3  0]\n",
      " [ 0  0  1  1  4 10  1  2  0]\n",
      " [ 0  0  0  0  3  1  1  2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.18      0.24      0.21        17\n",
      "           5       0.20      0.28      0.23        29\n",
      "           6       0.14      0.50      0.22        16\n",
      "           7       0.27      0.08      0.12        38\n",
      "           8       0.08      0.11      0.09        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.16       160\n",
      "   macro avg       0.10      0.13      0.10       160\n",
      "weighted avg       0.14      0.16      0.13       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "75 : epoch\n",
      "\n",
      "gloss : -54817281365.333336\n",
      "wd_loss : 5428.2734375\n",
      "cls_loss : 2.0091985066731772\n",
      "acc_t : 0.13599537478552925\n",
      "acc_s : 0.7667823897467719\n",
      "val_loss : 2.1945743560791016\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  0  2  2  2  0]\n",
      " [ 0  0  0  4  2  1  1  2  0]\n",
      " [ 0  0  0  6  3  2  2  3  0]\n",
      " [ 0  0  2  5  3  6  0  1  0]\n",
      " [ 0  1  1  9  6  5  1  6  0]\n",
      " [ 0  0  0  9  0  6  0  1  0]\n",
      " [ 0  0  0  7 12 11  3  5  0]\n",
      " [ 0  0  1  4  3  8  2  1  0]\n",
      " [ 0  0  0  3  2  0  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.10      0.29      0.15        17\n",
      "           5       0.19      0.21      0.20        29\n",
      "           6       0.15      0.38      0.21        16\n",
      "           7       0.25      0.08      0.12        38\n",
      "           8       0.05      0.05      0.05        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.08      0.11      0.08       160\n",
      "weighted avg       0.13      0.13      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "76 : epoch\n",
      "\n",
      "gloss : -43163385856.0\n",
      "wd_loss : 5557.401909722223\n",
      "cls_loss : 2.0072265201144748\n",
      "acc_t : 0.12962962521447075\n",
      "acc_s : 0.7853009435865614\n",
      "val_loss : 2.1906789779663085\n",
      "confusion_matrix\n",
      "[[ 0  0  0  4  0  1  1  2  0]\n",
      " [ 0  0  0  5  2  0  1  2  0]\n",
      " [ 0  0  0  7  3  3  2  1  0]\n",
      " [ 0  1  1  7  2  6  0  0  0]\n",
      " [ 0  1  1 13  7  3  1  3  0]\n",
      " [ 0  0  0 11  1  4  0  0  0]\n",
      " [ 0  1  0 15 12  4  4  2  0]\n",
      " [ 0  0  1  8  4  4  1  1  0]\n",
      " [ 0  1  0  3  0  0  1  2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.10      0.41      0.16        17\n",
      "           5       0.23      0.24      0.23        29\n",
      "           6       0.16      0.25      0.20        16\n",
      "           7       0.36      0.11      0.16        38\n",
      "           8       0.08      0.05      0.06        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.10      0.12      0.09       160\n",
      "weighted avg       0.16      0.14      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "77 : epoch\n",
      "\n",
      "gloss : -64483688448.0\n",
      "wd_loss : 5819.784722222223\n",
      "cls_loss : 2.00130738152398\n",
      "acc_t : 0.14351851410335964\n",
      "acc_s : 0.8148148324754503\n",
      "val_loss : 2.190103530883789\n",
      "confusion_matrix\n",
      "[[ 0  0  1  1  1  0  2  3  0]\n",
      " [ 0  0  0  2  2  3  2  1  0]\n",
      " [ 0  0  0  4  5  3  3  1  0]\n",
      " [ 0  0  2  4  2  7  1  1  0]\n",
      " [ 0  1  1  7 11  6  1  2  0]\n",
      " [ 0  0  0  9  1  5  1  0  0]\n",
      " [ 0  0  1  4 19  7  4  3  0]\n",
      " [ 0  0  1  4  4  7  2  1  0]\n",
      " [ 0  0  0  2  2  1  1  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.11      0.24      0.15        17\n",
      "           5       0.23      0.38      0.29        29\n",
      "           6       0.13      0.31      0.18        16\n",
      "           7       0.24      0.11      0.15        38\n",
      "           8       0.08      0.05      0.06        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.16       160\n",
      "   macro avg       0.09      0.12      0.09       160\n",
      "weighted avg       0.13      0.16      0.13       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "78 : epoch\n",
      "\n",
      "gloss : -74724627797.33333\n",
      "wd_loss : 6030.100694444444\n",
      "cls_loss : 2.0044278038872614\n",
      "acc_t : 0.1371527777777778\n",
      "acc_s : 0.7829861111111112\n",
      "val_loss : 2.195345163345337\n",
      "confusion_matrix\n",
      "[[ 0  0  1  2  1  2  0  2  0]\n",
      " [ 0  0  0  2  2  3  2  1  0]\n",
      " [ 0  0  1  3  3  4  3  2  0]\n",
      " [ 0  0  2  3  3  7  1  1  0]\n",
      " [ 0  1  2  5  9  9  1  2  0]\n",
      " [ 0  1  0  6  3  4  1  1  0]\n",
      " [ 0  0  1  2 16 13  3  3  0]\n",
      " [ 0  0  2  3  4  9  1  0  0]\n",
      " [ 0  0  0  2  0  2  1  2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.11      0.06      0.08        16\n",
      "           4       0.11      0.18      0.13        17\n",
      "           5       0.22      0.31      0.26        29\n",
      "           6       0.08      0.25      0.12        16\n",
      "           7       0.23      0.08      0.12        38\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.08      0.10      0.08       160\n",
      "weighted avg       0.12      0.12      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "79 : epoch\n",
      "\n",
      "gloss : -85410651249.77777\n",
      "wd_loss : 6220.532118055556\n",
      "cls_loss : 2.0049571990966797\n",
      "acc_t : 0.12962962521447075\n",
      "acc_s : 0.7818287213643392\n",
      "val_loss : 2.1994211196899416\n",
      "confusion_matrix\n",
      "[[ 0  0  0  1  1  3  1  2  0]\n",
      " [ 0  0  0  2  1  4  2  1  0]\n",
      " [ 0  0  1  4  3  6  2  0  0]\n",
      " [ 0  0  1  3  2  7  1  3  0]\n",
      " [ 0  0  2  5  6 12  1  3  0]\n",
      " [ 0  0  0  6  2  6  1  1  0]\n",
      " [ 0  0  1  1  7 21  4  4  0]\n",
      " [ 0  0  1  2  4  9  1  2  0]\n",
      " [ 0  1  0  1  0  2  1  2  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.17      0.06      0.09        16\n",
      "           4       0.12      0.18      0.14        17\n",
      "           5       0.23      0.21      0.22        29\n",
      "           6       0.09      0.38      0.14        16\n",
      "           7       0.29      0.11      0.15        38\n",
      "           8       0.11      0.11      0.11        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.11      0.11      0.09       160\n",
      "weighted avg       0.16      0.14      0.13       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "80 : epoch\n",
      "\n",
      "gloss : -81438536135.11111\n",
      "wd_loss : 6483.846354166667\n",
      "cls_loss : 1.9998122321234808\n",
      "acc_t : 0.12152777777777778\n",
      "acc_s : 0.7806712786356608\n",
      "val_loss : 2.1959933757781984\n",
      "confusion_matrix\n",
      "[[ 0  0  0  4  1  1  1  1  0]\n",
      " [ 0  0  0  3  1  3  1  2  0]\n",
      " [ 0  0  1  6  3  3  2  1  0]\n",
      " [ 0  1  0  7  2  4  0  3  0]\n",
      " [ 0  0  0  9  4  9  2  5  0]\n",
      " [ 0  0  0  7  3  4  1  1  0]\n",
      " [ 0  0  0  8  6 15  4  5  0]\n",
      " [ 0  0  0  5  4  7  1  2  0]\n",
      " [ 0  0  0  3  1  2  0  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       1.00      0.06      0.12        16\n",
      "           4       0.13      0.41      0.20        17\n",
      "           5       0.16      0.14      0.15        29\n",
      "           6       0.08      0.25      0.12        16\n",
      "           7       0.33      0.11      0.16        38\n",
      "           8       0.10      0.11      0.10        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.14       160\n",
      "   macro avg       0.20      0.12      0.09       160\n",
      "weighted avg       0.24      0.14      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "81 : epoch\n",
      "\n",
      "gloss : -87971361223.11111\n",
      "wd_loss : 6733.766493055556\n",
      "cls_loss : 1.9927342732747395\n",
      "acc_t : 0.1307870414521959\n",
      "acc_s : 0.7679398324754503\n",
      "val_loss : 2.1915776252746584\n",
      "confusion_matrix\n",
      "[[ 0  0  0  4  1  0  1  2  0]\n",
      " [ 0  0  0  4  1  4  0  1  0]\n",
      " [ 0  0  0  6  4  3  3  0  0]\n",
      " [ 0  2  0  6  2  4  0  3  0]\n",
      " [ 0  0  0 13  5  6  2  3  0]\n",
      " [ 0  0  0  7  3  3  1  2  0]\n",
      " [ 0  0  0 14  6 11  5  2  0]\n",
      " [ 0  0  1  4  4  6  2  2  0]\n",
      " [ 0  0  0  2  1  3  0  1  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.10      0.35      0.16        17\n",
      "           5       0.19      0.17      0.18        29\n",
      "           6       0.07      0.19      0.11        16\n",
      "           7       0.36      0.13      0.19        38\n",
      "           8       0.12      0.11      0.11        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.13       160\n",
      "   macro avg       0.09      0.11      0.08       160\n",
      "weighted avg       0.15      0.13      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "82 : epoch\n",
      "\n",
      "gloss : -75339770538.66667\n",
      "wd_loss : 6867.057725694444\n",
      "cls_loss : 1.9916706085205078\n",
      "acc_t : 0.13136573632558188\n",
      "acc_s : 0.8101851675245497\n",
      "val_loss : 2.189546537399292\n",
      "confusion_matrix\n",
      "[[ 0  0  1  2  2  0  1  2  0]\n",
      " [ 0  0  0  4  2  3  0  1  0]\n",
      " [ 0  0  0  4  5  3  4  0  0]\n",
      " [ 0  2  1  1  5  3  2  3  0]\n",
      " [ 0  0  4  6  8  4  3  4  0]\n",
      " [ 0  1  0  6  6  2  0  1  0]\n",
      " [ 0  1  1  4 19  3  7  3  0]\n",
      " [ 0  1  1  3  5  5  2  2  0]\n",
      " [ 0  0  1  1  0  4  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.03      0.06      0.04        17\n",
      "           5       0.15      0.28      0.20        29\n",
      "           6       0.07      0.12      0.09        16\n",
      "           7       0.35      0.18      0.24        38\n",
      "           8       0.12      0.11      0.11        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.08      0.08      0.08       160\n",
      "weighted avg       0.14      0.12      0.12       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "83 : epoch\n",
      "\n",
      "gloss : -103102232803.55556\n",
      "wd_loss : 6945.762152777777\n",
      "cls_loss : 1.997647179497613\n",
      "acc_t : 0.13194444444444445\n",
      "acc_s : 0.8298611111111112\n",
      "val_loss : 2.1922769069671633\n",
      "confusion_matrix\n",
      "[[ 0  0  0  2  2  2  1  1  0]\n",
      " [ 0  0  0  3  3  3  0  1  0]\n",
      " [ 0  2  0  2  5  3  3  1  0]\n",
      " [ 0  1  1  1  5  5  2  2  0]\n",
      " [ 0  0  2  5  6  8  3  5  0]\n",
      " [ 0  0  0  3  6  3  0  4  0]\n",
      " [ 0  1  1  3 18  5  7  3  0]\n",
      " [ 0  0  2  2  5  6  2  2  0]\n",
      " [ 0  1  0  0  0  5  1  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        16\n",
      "           4       0.05      0.06      0.05        17\n",
      "           5       0.12      0.21      0.15        29\n",
      "           6       0.07      0.19      0.11        16\n",
      "           7       0.37      0.18      0.25        38\n",
      "           8       0.11      0.11      0.11        19\n",
      "           9       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.12       160\n",
      "   macro avg       0.08      0.08      0.07       160\n",
      "weighted avg       0.13      0.12      0.11       160\n",
      "\n",
      "best_val_loss : 2.1851123332977296, epoch : 49\n",
      "\n",
      "84 : epoch\n"
     ]
    }
   ],
   "source": [
    "fe_VAL, dis_VAL, cls_VAL = train_val(source_VAL, target_VAL, val_VAL, 'VALENCE', 1000, 10, 10, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f84497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "AROUSAL label train and valiation\n",
      "device:  cuda:0\n",
      "\n",
      "1 : epoch\n",
      "\n",
      "gloss : -44.25697326660156\n",
      "wd_loss : 1.6806535720825195\n",
      "cls_loss : 2.1717569828033447\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.35546875\n",
      "val_loss : 2.1943238258361815\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1943238258361815, epoch : 1\n",
      "\n",
      "2 : epoch\n",
      "\n",
      "gloss : 10.759665489196777\n",
      "wd_loss : 3.664621114730835\n",
      "cls_loss : 2.1614749431610107\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.4140625\n",
      "val_loss : 2.194812536239624\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1943238258361815, epoch : 1\n",
      "\n",
      "3 : epoch\n",
      "\n",
      "gloss : 43.905330657958984\n",
      "wd_loss : 6.096080303192139\n",
      "cls_loss : 2.1535847187042236\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.435546875\n",
      "val_loss : 2.194874906539917\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1943238258361815, epoch : 1\n",
      "\n",
      "4 : epoch\n",
      "\n",
      "gloss : 92.28178405761719\n",
      "wd_loss : 10.38113784790039\n",
      "cls_loss : 2.148451089859009\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.439453125\n",
      "val_loss : 2.1950387954711914\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1943238258361815, epoch : 1\n",
      "\n",
      "5 : epoch\n",
      "\n",
      "gloss : -548.6296997070312\n",
      "wd_loss : 15.323498725891113\n",
      "cls_loss : 2.1441307067871094\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.458984375\n",
      "val_loss : 2.194556474685669\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1943238258361815, epoch : 1\n",
      "\n",
      "6 : epoch\n",
      "\n",
      "gloss : -1570.59130859375\n",
      "wd_loss : 19.706377029418945\n",
      "cls_loss : 2.1445860862731934\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.466796875\n",
      "val_loss : 2.1940540313720702\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1940540313720702, epoch : 6\n",
      "\n",
      "7 : epoch\n",
      "\n",
      "gloss : -9466.9501953125\n",
      "wd_loss : 27.0875244140625\n",
      "cls_loss : 2.139521360397339\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.48046875\n",
      "val_loss : 2.193589448928833\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.193589448928833, epoch : 7\n",
      "\n",
      "8 : epoch\n",
      "\n",
      "gloss : -14384.2861328125\n",
      "wd_loss : 34.235443115234375\n",
      "cls_loss : 2.1357421875\n",
      "acc_t : 0.212890625\n",
      "acc_s : 0.509765625\n",
      "val_loss : 2.19317045211792\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.19317045211792, epoch : 8\n",
      "\n",
      "9 : epoch\n",
      "\n",
      "gloss : -22271.83984375\n",
      "wd_loss : 42.26396560668945\n",
      "cls_loss : 2.13541841506958\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.50390625\n",
      "val_loss : 2.1929353713989257\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1929353713989257, epoch : 9\n",
      "\n",
      "10 : epoch\n",
      "\n",
      "gloss : -49431.12890625\n",
      "wd_loss : 54.091453552246094\n",
      "cls_loss : 2.1308915615081787\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.51953125\n",
      "val_loss : 2.1926711559295655\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1926711559295655, epoch : 10\n",
      "\n",
      "11 : epoch\n",
      "\n",
      "gloss : -50908.2578125\n",
      "wd_loss : 65.45372772216797\n",
      "cls_loss : 2.1298415660858154\n",
      "acc_t : 0.205078125\n",
      "acc_s : 0.517578125\n",
      "val_loss : 2.1922925472259522\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1922925472259522, epoch : 11\n",
      "\n",
      "12 : epoch\n",
      "\n",
      "gloss : -177387.125\n",
      "wd_loss : 78.5478286743164\n",
      "cls_loss : 2.128460168838501\n",
      "acc_t : 0.208984375\n",
      "acc_s : 0.521484375\n",
      "val_loss : 2.192064142227173\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.192064142227173, epoch : 12\n",
      "\n",
      "13 : epoch\n",
      "\n",
      "gloss : -188386.5625\n",
      "wd_loss : 93.45856475830078\n",
      "cls_loss : 2.1250393390655518\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.533203125\n",
      "val_loss : 2.1916642665863035\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.1916642665863035, epoch : 13\n",
      "\n",
      "14 : epoch\n",
      "\n",
      "gloss : -526132.5\n",
      "wd_loss : 105.99174499511719\n",
      "cls_loss : 2.125129461288452\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.515625\n",
      "val_loss : 2.19116587638855\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.19116587638855, epoch : 14\n",
      "\n",
      "15 : epoch\n",
      "\n",
      "gloss : -685359.625\n",
      "wd_loss : 124.71148681640625\n",
      "cls_loss : 2.1219542026519775\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.5390625\n",
      "val_loss : 2.1913509368896484\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.19116587638855, epoch : 14\n",
      "\n",
      "16 : epoch\n",
      "\n",
      "gloss : -1137183.25\n",
      "wd_loss : 139.6427001953125\n",
      "cls_loss : 2.122894287109375\n",
      "acc_t : 0.216796875\n",
      "acc_s : 0.537109375\n",
      "val_loss : 2.1905078411102297\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1905078411102297, epoch : 16\n",
      "\n",
      "17 : epoch\n",
      "\n",
      "gloss : -1803467.75\n",
      "wd_loss : 156.01307678222656\n",
      "cls_loss : 2.118515729904175\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.1902629375457763\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1902629375457763, epoch : 17\n",
      "\n",
      "18 : epoch\n",
      "\n",
      "gloss : -1904690.375\n",
      "wd_loss : 178.89999389648438\n",
      "cls_loss : 2.1177010536193848\n",
      "acc_t : 0.2109375\n",
      "acc_s : 0.548828125\n",
      "val_loss : 2.1906036376953124\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1902629375457763, epoch : 17\n",
      "\n",
      "19 : epoch\n",
      "\n",
      "gloss : -3932201.0\n",
      "wd_loss : 199.59820556640625\n",
      "cls_loss : 2.1179490089416504\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.189613914489746\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.189613914489746, epoch : 19\n",
      "\n",
      "20 : epoch\n",
      "\n",
      "gloss : -4302099.0\n",
      "wd_loss : 229.43002319335938\n",
      "cls_loss : 2.1132030487060547\n",
      "acc_t : 0.220703125\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.18949408531189\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.18949408531189, epoch : 20\n",
      "\n",
      "21 : epoch\n",
      "\n",
      "gloss : -5907570.0\n",
      "wd_loss : 248.93551635742188\n",
      "cls_loss : 2.1097848415374756\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.5625\n",
      "val_loss : 2.1902676582336427\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.18949408531189, epoch : 20\n",
      "\n",
      "22 : epoch\n",
      "\n",
      "gloss : -7949178.0\n",
      "wd_loss : 280.4117431640625\n",
      "cls_loss : 2.1120407581329346\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.556640625\n",
      "val_loss : 2.1900465965270994\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.18949408531189, epoch : 20\n",
      "\n",
      "23 : epoch\n",
      "\n",
      "gloss : -8814293.0\n",
      "wd_loss : 306.0809631347656\n",
      "cls_loss : 2.1095778942108154\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.5625\n",
      "val_loss : 2.1891082286834718\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1891082286834718, epoch : 23\n",
      "\n",
      "24 : epoch\n",
      "\n",
      "gloss : -16668408.0\n",
      "wd_loss : 339.3037109375\n",
      "cls_loss : 2.1063027381896973\n",
      "acc_t : 0.23046875\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.189587688446045\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1891082286834718, epoch : 23\n",
      "\n",
      "25 : epoch\n",
      "\n",
      "gloss : -12226445.0\n",
      "wd_loss : 371.28717041015625\n",
      "cls_loss : 2.1039938926696777\n",
      "acc_t : 0.234375\n",
      "acc_s : 0.5546875\n",
      "val_loss : 2.1899845123291017\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.1891082286834718, epoch : 23\n",
      "\n",
      "26 : epoch\n",
      "\n",
      "gloss : -21219276.0\n",
      "wd_loss : 413.22900390625\n",
      "cls_loss : 2.1049795150756836\n",
      "acc_t : 0.220703125\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.188514804840088\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.188514804840088, epoch : 26\n",
      "\n",
      "27 : epoch\n",
      "\n",
      "gloss : -32248088.0\n",
      "wd_loss : 441.89849853515625\n",
      "cls_loss : 2.1010241508483887\n",
      "acc_t : 0.23046875\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.187823438644409\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.187823438644409, epoch : 27\n",
      "\n",
      "28 : epoch\n",
      "\n",
      "gloss : -23821694.0\n",
      "wd_loss : 483.67755126953125\n",
      "cls_loss : 2.1006906032562256\n",
      "acc_t : 0.232421875\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.188114070892334\n",
      "acc_val : 0.20625\n",
      "best_val_loss : 2.187823438644409, epoch : 27\n",
      "\n",
      "29 : epoch\n",
      "\n",
      "gloss : -43350784.0\n",
      "wd_loss : 508.9742431640625\n",
      "cls_loss : 2.1014091968536377\n",
      "acc_t : 0.22265625\n",
      "acc_s : 0.5546875\n",
      "val_loss : 2.1876765727996825\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1876765727996825, epoch : 29\n",
      "\n",
      "30 : epoch\n",
      "\n",
      "gloss : -34327312.0\n",
      "wd_loss : 545.6715698242188\n",
      "cls_loss : 2.099832057952881\n",
      "acc_t : 0.224609375\n",
      "acc_s : 0.544921875\n",
      "val_loss : 2.186820650100708\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.186820650100708, epoch : 30\n",
      "\n",
      "31 : epoch\n",
      "\n",
      "gloss : -89027432.0\n",
      "wd_loss : 579.386474609375\n",
      "cls_loss : 2.098795175552368\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.548828125\n",
      "val_loss : 2.1872323036193846\n",
      "acc_val : 0.209375\n",
      "best_val_loss : 2.186820650100708, epoch : 30\n",
      "\n",
      "32 : epoch\n",
      "\n",
      "gloss : -90890600.0\n",
      "wd_loss : 631.9779052734375\n",
      "cls_loss : 2.0969290733337402\n",
      "acc_t : 0.220703125\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.1864951133728026\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1864951133728026, epoch : 32\n",
      "\n",
      "33 : epoch\n",
      "\n",
      "gloss : -52482552.0\n",
      "wd_loss : 674.7828979492188\n",
      "cls_loss : 2.097555160522461\n",
      "acc_t : 0.21875\n",
      "acc_s : 0.544921875\n",
      "val_loss : 2.18603777885437\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "34 : epoch\n",
      "\n",
      "gloss : -153342864.0\n",
      "wd_loss : 739.9545288085938\n",
      "cls_loss : 2.0903992652893066\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.568359375\n",
      "val_loss : 2.1870824337005614\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "35 : epoch\n",
      "\n",
      "gloss : -187592064.0\n",
      "wd_loss : 766.8016357421875\n",
      "cls_loss : 2.0909852981567383\n",
      "acc_t : 0.212890625\n",
      "acc_s : 0.5546875\n",
      "val_loss : 2.187416362762451\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "36 : epoch\n",
      "\n",
      "gloss : -164343536.0\n",
      "wd_loss : 793.5061645507812\n",
      "cls_loss : 2.0938806533813477\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.5390625\n",
      "val_loss : 2.1861587524414063\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "37 : epoch\n",
      "\n",
      "gloss : -210027200.0\n",
      "wd_loss : 839.057861328125\n",
      "cls_loss : 2.0921449661254883\n",
      "acc_t : 0.208984375\n",
      "acc_s : 0.552734375\n",
      "val_loss : 2.187321186065674\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "38 : epoch\n",
      "\n",
      "gloss : -345980928.0\n",
      "wd_loss : 902.6509399414062\n",
      "cls_loss : 2.085860013961792\n",
      "acc_t : 0.220703125\n",
      "acc_s : 0.56640625\n",
      "val_loss : 2.1882311344146728\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "39 : epoch\n",
      "\n",
      "gloss : -292415072.0\n",
      "wd_loss : 996.3828735351562\n",
      "cls_loss : 2.0827343463897705\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.1892731189727783\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "40 : epoch\n",
      "\n",
      "gloss : -339462400.0\n",
      "wd_loss : 1025.7542724609375\n",
      "cls_loss : 2.0899949073791504\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.1884780406951903\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "41 : epoch\n",
      "\n",
      "gloss : -363771232.0\n",
      "wd_loss : 1116.97509765625\n",
      "cls_loss : 2.085041046142578\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.529296875\n",
      "val_loss : 2.187525987625122\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "42 : epoch\n",
      "\n",
      "gloss : -462631776.0\n",
      "wd_loss : 1167.1229248046875\n",
      "cls_loss : 2.0812253952026367\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.548828125\n",
      "val_loss : 2.188612461090088\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "43 : epoch\n",
      "\n",
      "gloss : -653155712.0\n",
      "wd_loss : 1178.385986328125\n",
      "cls_loss : 2.080747127532959\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.5625\n",
      "val_loss : 2.1887120246887206\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "44 : epoch\n",
      "\n",
      "gloss : -405709664.0\n",
      "wd_loss : 1284.217529296875\n",
      "cls_loss : 2.081505537033081\n",
      "acc_t : 0.208984375\n",
      "acc_s : 0.572265625\n",
      "val_loss : 2.188829469680786\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "45 : epoch\n",
      "\n",
      "gloss : -514438080.0\n",
      "wd_loss : 1298.9130859375\n",
      "cls_loss : 2.085878849029541\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.552734375\n",
      "val_loss : 2.1890098094940185\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "46 : epoch\n",
      "\n",
      "gloss : -894284416.0\n",
      "wd_loss : 1378.2479248046875\n",
      "cls_loss : 2.0781009197235107\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.548828125\n",
      "val_loss : 2.18773832321167\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.18603777885437, epoch : 33\n",
      "\n",
      "47 : epoch\n",
      "\n",
      "gloss : -946891136.0\n",
      "wd_loss : 1463.95166015625\n",
      "cls_loss : 2.072180986404419\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.54296875\n",
      "val_loss : 2.1853779792785644\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1853779792785644, epoch : 47\n",
      "\n",
      "48 : epoch\n",
      "\n",
      "gloss : -930293248.0\n",
      "wd_loss : 1504.4329833984375\n",
      "cls_loss : 2.071530818939209\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.5546875\n",
      "val_loss : 2.1852118492126467\n",
      "acc_val : 0.20625\n",
      "best_val_loss : 2.1852118492126467, epoch : 48\n",
      "\n",
      "49 : epoch\n",
      "\n",
      "gloss : -882487168.0\n",
      "wd_loss : 1598.71533203125\n",
      "cls_loss : 2.075160026550293\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.54296875\n",
      "val_loss : 2.1855841159820555\n",
      "acc_val : 0.21875\n",
      "best_val_loss : 2.1852118492126467, epoch : 48\n",
      "\n",
      "50 : epoch\n",
      "\n",
      "gloss : -836756992.0\n",
      "wd_loss : 1602.2425537109375\n",
      "cls_loss : 2.079195737838745\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.185839319229126\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1852118492126467, epoch : 48\n",
      "\n",
      "51 : epoch\n",
      "\n",
      "gloss : -1368605696.0\n",
      "wd_loss : 1668.4659423828125\n",
      "cls_loss : 2.075373888015747\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.1838425159454347\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1838425159454347, epoch : 51\n",
      "\n",
      "52 : epoch\n",
      "\n",
      "gloss : -1313522048.0\n",
      "wd_loss : 1795.7369384765625\n",
      "cls_loss : 2.0699195861816406\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.5234375\n",
      "val_loss : 2.181487560272217\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.181487560272217, epoch : 52\n",
      "\n",
      "53 : epoch\n",
      "\n",
      "gloss : -2073869440.0\n",
      "wd_loss : 1922.8643798828125\n",
      "cls_loss : 2.067777633666992\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.5390625\n",
      "val_loss : 2.1814849853515623\n",
      "acc_val : 0.209375\n",
      "best_val_loss : 2.1814849853515623, epoch : 53\n",
      "\n",
      "54 : epoch\n",
      "\n",
      "gloss : -1561705984.0\n",
      "wd_loss : 1913.99072265625\n",
      "cls_loss : 2.0712153911590576\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.537109375\n",
      "val_loss : 2.1819408416748045\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1814849853515623, epoch : 53\n",
      "\n",
      "55 : epoch\n",
      "\n",
      "gloss : -1529977216.0\n",
      "wd_loss : 2166.917236328125\n",
      "cls_loss : 2.065931558609009\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.548828125\n",
      "val_loss : 2.180162954330444\n",
      "acc_val : 0.20625\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "56 : epoch\n",
      "\n",
      "gloss : -2430795008.0\n",
      "wd_loss : 2187.326171875\n",
      "cls_loss : 2.0645244121551514\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.53515625\n",
      "val_loss : 2.1813313961029053\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "57 : epoch\n",
      "\n",
      "gloss : -3087289856.0\n",
      "wd_loss : 2165.8525390625\n",
      "cls_loss : 2.0666921138763428\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.529296875\n",
      "val_loss : 2.1824324131011963\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "58 : epoch\n",
      "\n",
      "gloss : -4134612480.0\n",
      "wd_loss : 2402.201171875\n",
      "cls_loss : 2.0635533332824707\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.544921875\n",
      "val_loss : 2.1834485054016115\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "59 : epoch\n",
      "\n",
      "gloss : -5138362368.0\n",
      "wd_loss : 2464.4345703125\n",
      "cls_loss : 2.0626890659332275\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.1835103034973145\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "60 : epoch\n",
      "\n",
      "gloss : -4406882304.0\n",
      "wd_loss : 2493.00390625\n",
      "cls_loss : 2.061094284057617\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.537109375\n",
      "val_loss : 2.182460975646973\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "61 : epoch\n",
      "\n",
      "gloss : -4451115008.0\n",
      "wd_loss : 2665.32470703125\n",
      "cls_loss : 2.053415536880493\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.533203125\n",
      "val_loss : 2.181010437011719\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "62 : epoch\n",
      "\n",
      "gloss : -5615181824.0\n",
      "wd_loss : 2735.720703125\n",
      "cls_loss : 2.055751323699951\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.509765625\n",
      "val_loss : 2.1806397438049316\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "63 : epoch\n",
      "\n",
      "gloss : -7581133312.0\n",
      "wd_loss : 2851.69091796875\n",
      "cls_loss : 2.053755521774292\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.525390625\n",
      "val_loss : 2.1816452980041503\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "64 : epoch\n",
      "\n",
      "gloss : -4024937728.0\n",
      "wd_loss : 2957.398193359375\n",
      "cls_loss : 2.0544071197509766\n",
      "acc_t : 0.208984375\n",
      "acc_s : 0.544921875\n",
      "val_loss : 2.1809621334075926\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.180162954330444, epoch : 55\n",
      "\n",
      "65 : epoch\n",
      "\n",
      "gloss : -8648078336.0\n",
      "wd_loss : 3183.04638671875\n",
      "cls_loss : 2.0511603355407715\n",
      "acc_t : 0.216796875\n",
      "acc_s : 0.57421875\n",
      "val_loss : 2.1801183223724365\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.1801183223724365, epoch : 65\n",
      "\n",
      "66 : epoch\n",
      "\n",
      "gloss : -10368894976.0\n",
      "wd_loss : 3312.770751953125\n",
      "cls_loss : 2.0509512424468994\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.1792758464813233\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1792758464813233, epoch : 66\n",
      "\n",
      "67 : epoch\n",
      "\n",
      "gloss : -12096145408.0\n",
      "wd_loss : 3280.192626953125\n",
      "cls_loss : 2.046355724334717\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.1784725189208984\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1784725189208984, epoch : 67\n",
      "\n",
      "68 : epoch\n",
      "\n",
      "gloss : -9777182720.0\n",
      "wd_loss : 3354.24365234375\n",
      "cls_loss : 2.046083450317383\n",
      "acc_t : 0.21875\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.1769275665283203\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "69 : epoch\n",
      "\n",
      "gloss : -9009786880.0\n",
      "wd_loss : 3477.828125\n",
      "cls_loss : 2.041245698928833\n",
      "acc_t : 0.224609375\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.1775278091430663\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "70 : epoch\n",
      "\n",
      "gloss : -12077630464.0\n",
      "wd_loss : 3588.779052734375\n",
      "cls_loss : 2.047609329223633\n",
      "acc_t : 0.208984375\n",
      "acc_s : 0.544921875\n",
      "val_loss : 2.1794029235839845\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "71 : epoch\n",
      "\n",
      "gloss : -11409518592.0\n",
      "wd_loss : 3840.681396484375\n",
      "cls_loss : 2.0450894832611084\n",
      "acc_t : 0.212890625\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.1807697296142576\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "72 : epoch\n",
      "\n",
      "gloss : -15663556608.0\n",
      "wd_loss : 4076.98583984375\n",
      "cls_loss : 2.0386874675750732\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.576171875\n",
      "val_loss : 2.1809834003448487\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "73 : epoch\n",
      "\n",
      "gloss : -16594180096.0\n",
      "wd_loss : 4274.26025390625\n",
      "cls_loss : 2.031071901321411\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.5859375\n",
      "val_loss : 2.1813250541687013\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "74 : epoch\n",
      "\n",
      "gloss : -15739564032.0\n",
      "wd_loss : 4501.1005859375\n",
      "cls_loss : 2.023857355117798\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.58984375\n",
      "val_loss : 2.180923509597778\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "75 : epoch\n",
      "\n",
      "gloss : -19846817792.0\n",
      "wd_loss : 4555.05810546875\n",
      "cls_loss : 2.0271759033203125\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.179411220550537\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "76 : epoch\n",
      "\n",
      "gloss : -13433818112.0\n",
      "wd_loss : 4768.388671875\n",
      "cls_loss : 2.030682325363159\n",
      "acc_t : 0.2109375\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.1784825801849363\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.1769275665283203, epoch : 68\n",
      "\n",
      "77 : epoch\n",
      "\n",
      "gloss : -30426548224.0\n",
      "wd_loss : 4930.0146484375\n",
      "cls_loss : 2.0278921127319336\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.176313018798828\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.176313018798828, epoch : 77\n",
      "\n",
      "78 : epoch\n",
      "\n",
      "gloss : -26844903424.0\n",
      "wd_loss : 4907.78515625\n",
      "cls_loss : 2.023871898651123\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.57421875\n",
      "val_loss : 2.176758575439453\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.176313018798828, epoch : 77\n",
      "\n",
      "79 : epoch\n",
      "\n",
      "gloss : -27786098688.0\n",
      "wd_loss : 4938.7197265625\n",
      "cls_loss : 2.0284461975097656\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.587890625\n",
      "val_loss : 2.178296136856079\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.176313018798828, epoch : 77\n",
      "\n",
      "80 : epoch\n",
      "\n",
      "gloss : -34170066944.0\n",
      "wd_loss : 5074.125\n",
      "cls_loss : 2.0313830375671387\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.580078125\n",
      "val_loss : 2.1759884357452393\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1759884357452393, epoch : 80\n",
      "\n",
      "81 : epoch\n",
      "\n",
      "gloss : -35141574656.0\n",
      "wd_loss : 5335.2373046875\n",
      "cls_loss : 2.0244076251983643\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.57421875\n",
      "val_loss : 2.1718042850494386\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1718042850494386, epoch : 81\n",
      "\n",
      "82 : epoch\n",
      "\n",
      "gloss : -46291431424.0\n",
      "wd_loss : 5758.4189453125\n",
      "cls_loss : 2.019700288772583\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.56640625\n",
      "val_loss : 2.1717942237854\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "83 : epoch\n",
      "\n",
      "gloss : -40977788928.0\n",
      "wd_loss : 5589.59375\n",
      "cls_loss : 2.0195395946502686\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.1737475395202637\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "84 : epoch\n",
      "\n",
      "gloss : -49067253760.0\n",
      "wd_loss : 5617.314453125\n",
      "cls_loss : 2.0245604515075684\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.177072811126709\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "85 : epoch\n",
      "\n",
      "gloss : -55346462720.0\n",
      "wd_loss : 5594.4609375\n",
      "cls_loss : 2.0256476402282715\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.1776363849639893\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "86 : epoch\n",
      "\n",
      "gloss : -41867722752.0\n",
      "wd_loss : 6150.7841796875\n",
      "cls_loss : 2.0166420936584473\n",
      "acc_t : 0.16796875\n",
      "acc_s : 0.5625\n",
      "val_loss : 2.177356481552124\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "87 : epoch\n",
      "\n",
      "gloss : -50440626176.0\n",
      "wd_loss : 6307.66845703125\n",
      "cls_loss : 2.01139760017395\n",
      "acc_t : 0.16796875\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.175843048095703\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "88 : epoch\n",
      "\n",
      "gloss : -54216179712.0\n",
      "wd_loss : 6423.80859375\n",
      "cls_loss : 2.0106563568115234\n",
      "acc_t : 0.166015625\n",
      "acc_s : 0.556640625\n",
      "val_loss : 2.176434278488159\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "89 : epoch\n",
      "\n",
      "gloss : -71918493696.0\n",
      "wd_loss : 6291.76904296875\n",
      "cls_loss : 2.010542154312134\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.56640625\n",
      "val_loss : 2.1764766216278075\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "90 : epoch\n",
      "\n",
      "gloss : -39602827264.0\n",
      "wd_loss : 6683.15625\n",
      "cls_loss : 2.005765199661255\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.5703125\n",
      "val_loss : 2.175726890563965\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "91 : epoch\n",
      "\n",
      "gloss : -87271964672.0\n",
      "wd_loss : 6804.34912109375\n",
      "cls_loss : 2.006840229034424\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.564453125\n",
      "val_loss : 2.17429895401001\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "92 : epoch\n",
      "\n",
      "gloss : -51395145728.0\n",
      "wd_loss : 7007.9677734375\n",
      "cls_loss : 2.0063722133636475\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.58203125\n",
      "val_loss : 2.175262975692749\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "93 : epoch\n",
      "\n",
      "gloss : -88840142848.0\n",
      "wd_loss : 7292.2978515625\n",
      "cls_loss : 2.00726580619812\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.57421875\n",
      "val_loss : 2.1735056400299073\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "94 : epoch\n",
      "\n",
      "gloss : -91639013376.0\n",
      "wd_loss : 7411.9296875\n",
      "cls_loss : 2.0046210289001465\n",
      "acc_t : 0.208984375\n",
      "acc_s : 0.58203125\n",
      "val_loss : 2.1728327751159666\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.1717942237854, epoch : 82\n",
      "\n",
      "95 : epoch\n",
      "\n",
      "gloss : -97787281408.0\n",
      "wd_loss : 7992.53369140625\n",
      "cls_loss : 2.000317335128784\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.57421875\n",
      "val_loss : 2.1717746257781982\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1717746257781982, epoch : 95\n",
      "\n",
      "96 : epoch\n",
      "\n",
      "gloss : -109871833088.0\n",
      "wd_loss : 7977.3330078125\n",
      "cls_loss : 2.0035996437072754\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.578125\n",
      "val_loss : 2.169268798828125\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.169268798828125, epoch : 96\n",
      "\n",
      "97 : epoch\n",
      "\n",
      "gloss : -93102637056.0\n",
      "wd_loss : 7882.4814453125\n",
      "cls_loss : 2.0018348693847656\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.166287422180176\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.166287422180176, epoch : 97\n",
      "\n",
      "98 : epoch\n",
      "\n",
      "gloss : -123942649856.0\n",
      "wd_loss : 8370.1982421875\n",
      "cls_loss : 2.000617027282715\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.1653886795043946\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "99 : epoch\n",
      "\n",
      "gloss : -105404342272.0\n",
      "wd_loss : 8833.087890625\n",
      "cls_loss : 1.9982326030731201\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.168042802810669\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "100 : epoch\n",
      "\n",
      "gloss : -122873659392.0\n",
      "wd_loss : 8858.326171875\n",
      "cls_loss : 1.9990180730819702\n",
      "acc_t : 0.171875\n",
      "acc_s : 0.556640625\n",
      "val_loss : 2.1708332538604735\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "101 : epoch\n",
      "\n",
      "gloss : -116070793216.0\n",
      "wd_loss : 8780.7216796875\n",
      "cls_loss : 2.0018231868743896\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.56640625\n",
      "val_loss : 2.173465538024902\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "102 : epoch\n",
      "\n",
      "gloss : -189909123072.0\n",
      "wd_loss : 9026.05859375\n",
      "cls_loss : 1.9988441467285156\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.59375\n",
      "val_loss : 2.1735880851745604\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "103 : epoch\n",
      "\n",
      "gloss : -190634393600.0\n",
      "wd_loss : 9205.697265625\n",
      "cls_loss : 1.9921398162841797\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.587890625\n",
      "val_loss : 2.1731393814086912\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "104 : epoch\n",
      "\n",
      "gloss : -225642102784.0\n",
      "wd_loss : 9429.974609375\n",
      "cls_loss : 1.9911563396453857\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.5703125\n",
      "val_loss : 2.1731911182403563\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "105 : epoch\n",
      "\n",
      "gloss : -245340618752.0\n",
      "wd_loss : 9455.931640625\n",
      "cls_loss : 1.991050124168396\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.552734375\n",
      "val_loss : 2.1729500770568846\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "106 : epoch\n",
      "\n",
      "gloss : -196497571840.0\n",
      "wd_loss : 9288.4765625\n",
      "cls_loss : 2.000087022781372\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.509765625\n",
      "val_loss : 2.1720795154571535\n",
      "acc_val : 0.15625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "107 : epoch\n",
      "\n",
      "gloss : -158116708352.0\n",
      "wd_loss : 9571.87890625\n",
      "cls_loss : 2.002264976501465\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.521484375\n",
      "val_loss : 2.1731695175170898\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "108 : epoch\n",
      "\n",
      "gloss : -244867825664.0\n",
      "wd_loss : 9672.3193359375\n",
      "cls_loss : 1.9922118186950684\n",
      "acc_t : 0.205078125\n",
      "acc_s : 0.5390625\n",
      "val_loss : 2.171195077896118\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "109 : epoch\n",
      "\n",
      "gloss : -159261327360.0\n",
      "wd_loss : 10529.8515625\n",
      "cls_loss : 1.9850932359695435\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.170657920837402\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "110 : epoch\n",
      "\n",
      "gloss : -226758328320.0\n",
      "wd_loss : 10595.12109375\n",
      "cls_loss : 1.9851405620574951\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.5703125\n",
      "val_loss : 2.1690975189208985\n",
      "acc_val : 0.14375\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "111 : epoch\n",
      "\n",
      "gloss : -260205346816.0\n",
      "wd_loss : 10784.4375\n",
      "cls_loss : 1.9816495180130005\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.58984375\n",
      "val_loss : 2.1665331363677978\n",
      "acc_val : 0.1375\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "112 : epoch\n",
      "\n",
      "gloss : -330780246016.0\n",
      "wd_loss : 11167.080078125\n",
      "cls_loss : 1.9749444723129272\n",
      "acc_t : 0.205078125\n",
      "acc_s : 0.599609375\n",
      "val_loss : 2.167049503326416\n",
      "acc_val : 0.13125\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "113 : epoch\n",
      "\n",
      "gloss : -327359889408.0\n",
      "wd_loss : 11592.7919921875\n",
      "cls_loss : 1.9772112369537354\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.587890625\n",
      "val_loss : 2.167246961593628\n",
      "acc_val : 0.128125\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "114 : epoch\n",
      "\n",
      "gloss : -251323219968.0\n",
      "wd_loss : 11564.8671875\n",
      "cls_loss : 1.979797601699829\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.552734375\n",
      "val_loss : 2.1681260585784914\n",
      "acc_val : 0.13125\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "115 : epoch\n",
      "\n",
      "gloss : -400841736192.0\n",
      "wd_loss : 11797.3115234375\n",
      "cls_loss : 1.9829998016357422\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.533203125\n",
      "val_loss : 2.1692795753479004\n",
      "acc_val : 0.128125\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "116 : epoch\n",
      "\n",
      "gloss : -364578340864.0\n",
      "wd_loss : 11388.1923828125\n",
      "cls_loss : 1.9868066310882568\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.517578125\n",
      "val_loss : 2.169514799118042\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "117 : epoch\n",
      "\n",
      "gloss : -444563259392.0\n",
      "wd_loss : 11815.62890625\n",
      "cls_loss : 1.9791628122329712\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.53515625\n",
      "val_loss : 2.1679295539855956\n",
      "acc_val : 0.14375\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "118 : epoch\n",
      "\n",
      "gloss : -609303199744.0\n",
      "wd_loss : 12890.826171875\n",
      "cls_loss : 1.9694576263427734\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.166822147369385\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "119 : epoch\n",
      "\n",
      "gloss : -467441811456.0\n",
      "wd_loss : 13198.2451171875\n",
      "cls_loss : 1.9657557010650635\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.564453125\n",
      "val_loss : 2.167776346206665\n",
      "acc_val : 0.15625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "120 : epoch\n",
      "\n",
      "gloss : -434384273408.0\n",
      "wd_loss : 13061.1142578125\n",
      "cls_loss : 1.9685266017913818\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.564453125\n",
      "val_loss : 2.166947364807129\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1653886795043946, epoch : 98\n",
      "\n",
      "121 : epoch\n",
      "\n",
      "gloss : -592889970688.0\n",
      "wd_loss : 13176.28125\n",
      "cls_loss : 1.9629074335098267\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.5859375\n",
      "val_loss : 2.164372968673706\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.164372968673706, epoch : 121\n",
      "\n",
      "122 : epoch\n",
      "\n",
      "gloss : -523227725824.0\n",
      "wd_loss : 14167.984375\n",
      "cls_loss : 1.955957055091858\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.552734375\n",
      "val_loss : 2.162087249755859\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.162087249755859, epoch : 122\n",
      "\n",
      "123 : epoch\n",
      "\n",
      "gloss : -657606705152.0\n",
      "wd_loss : 14159.427734375\n",
      "cls_loss : 1.954771637916565\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.5546875\n",
      "val_loss : 2.1627654075622558\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.162087249755859, epoch : 122\n",
      "\n",
      "124 : epoch\n",
      "\n",
      "gloss : -511303385088.0\n",
      "wd_loss : 14177.046875\n",
      "cls_loss : 1.9608852863311768\n",
      "acc_t : 0.216796875\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.164588689804077\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.162087249755859, epoch : 122\n",
      "\n",
      "125 : epoch\n",
      "\n",
      "gloss : -655650324480.0\n",
      "wd_loss : 14654.064453125\n",
      "cls_loss : 1.9646726846694946\n",
      "acc_t : 0.2109375\n",
      "acc_s : 0.552734375\n",
      "val_loss : 2.1630141735076904\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.162087249755859, epoch : 122\n",
      "\n",
      "126 : epoch\n",
      "\n",
      "gloss : -377775390720.0\n",
      "wd_loss : 15097.876953125\n",
      "cls_loss : 1.9640339612960815\n",
      "acc_t : 0.2109375\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.1603822231292726\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1603822231292726, epoch : 126\n",
      "\n",
      "127 : epoch\n",
      "\n",
      "gloss : -819142000640.0\n",
      "wd_loss : 15516.58203125\n",
      "cls_loss : 1.966691493988037\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.53125\n",
      "val_loss : 2.1585349559783937\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1585349559783937, epoch : 127\n",
      "\n",
      "128 : epoch\n",
      "\n",
      "gloss : -1015222829056.0\n",
      "wd_loss : 15922.873046875\n",
      "cls_loss : 1.9650976657867432\n",
      "acc_t : 0.212890625\n",
      "acc_s : 0.513671875\n",
      "val_loss : 2.158884859085083\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1585349559783937, epoch : 127\n",
      "\n",
      "129 : epoch\n",
      "\n",
      "gloss : -722566053888.0\n",
      "wd_loss : 15814.13671875\n",
      "cls_loss : 1.9608862400054932\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.54296875\n",
      "val_loss : 2.155999708175659\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.155999708175659, epoch : 129\n",
      "\n",
      "130 : epoch\n",
      "\n",
      "gloss : -696133091328.0\n",
      "wd_loss : 16197.6220703125\n",
      "cls_loss : 1.9551879167556763\n",
      "acc_t : 0.212890625\n",
      "acc_s : 0.529296875\n",
      "val_loss : 2.1534451961517336\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1534451961517336, epoch : 130\n",
      "\n",
      "131 : epoch\n",
      "\n",
      "gloss : -957408149504.0\n",
      "wd_loss : 16790.072265625\n",
      "cls_loss : 1.9489887952804565\n",
      "acc_t : 0.224609375\n",
      "acc_s : 0.552734375\n",
      "val_loss : 2.149361085891724\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "132 : epoch\n",
      "\n",
      "gloss : -964090789888.0\n",
      "wd_loss : 16865.74609375\n",
      "cls_loss : 1.9436683654785156\n",
      "acc_t : 0.212890625\n",
      "acc_s : 0.556640625\n",
      "val_loss : 2.1495418548583984\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "133 : epoch\n",
      "\n",
      "gloss : -1015971184640.0\n",
      "wd_loss : 17212.6171875\n",
      "cls_loss : 1.943142294883728\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.572265625\n",
      "val_loss : 2.151289176940918\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "134 : epoch\n",
      "\n",
      "gloss : -946074091520.0\n",
      "wd_loss : 17952.015625\n",
      "cls_loss : 1.9451979398727417\n",
      "acc_t : 0.2109375\n",
      "acc_s : 0.578125\n",
      "val_loss : 2.151938247680664\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "135 : epoch\n",
      "\n",
      "gloss : -796139388928.0\n",
      "wd_loss : 18489.9375\n",
      "cls_loss : 1.9469720125198364\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.155829095840454\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "136 : epoch\n",
      "\n",
      "gloss : -1175318429696.0\n",
      "wd_loss : 18606.86328125\n",
      "cls_loss : 1.945664405822754\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.15953688621521\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "137 : epoch\n",
      "\n",
      "gloss : -1158104088576.0\n",
      "wd_loss : 18715.72265625\n",
      "cls_loss : 1.9451696872711182\n",
      "acc_t : 0.21875\n",
      "acc_s : 0.52734375\n",
      "val_loss : 2.161050319671631\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "138 : epoch\n",
      "\n",
      "gloss : -1195924914176.0\n",
      "wd_loss : 19166.533203125\n",
      "cls_loss : 1.9429593086242676\n",
      "acc_t : 0.220703125\n",
      "acc_s : 0.537109375\n",
      "val_loss : 2.158893918991089\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "139 : epoch\n",
      "\n",
      "gloss : -1401690259456.0\n",
      "wd_loss : 18935.314453125\n",
      "cls_loss : 1.9394257068634033\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.509765625\n",
      "val_loss : 2.1578112602233888\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "140 : epoch\n",
      "\n",
      "gloss : -1443495673856.0\n",
      "wd_loss : 18631.716796875\n",
      "cls_loss : 1.93247389793396\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.541015625\n",
      "val_loss : 2.157072067260742\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "141 : epoch\n",
      "\n",
      "gloss : -1821201530880.0\n",
      "wd_loss : 18291.818359375\n",
      "cls_loss : 1.931993842124939\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.53125\n",
      "val_loss : 2.155248785018921\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "142 : epoch\n",
      "\n",
      "gloss : -1234278285312.0\n",
      "wd_loss : 19756.134765625\n",
      "cls_loss : 1.9286057949066162\n",
      "acc_t : 0.205078125\n",
      "acc_s : 0.52734375\n",
      "val_loss : 2.154928731918335\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "143 : epoch\n",
      "\n",
      "gloss : -1091568205824.0\n",
      "wd_loss : 19953.935546875\n",
      "cls_loss : 1.9304218292236328\n",
      "acc_t : 0.205078125\n",
      "acc_s : 0.56640625\n",
      "val_loss : 2.155895805358887\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "144 : epoch\n",
      "\n",
      "gloss : -1441277673472.0\n",
      "wd_loss : 20889.169921875\n",
      "cls_loss : 1.933170199394226\n",
      "acc_t : 0.21875\n",
      "acc_s : 0.556640625\n",
      "val_loss : 2.1562721729278564\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "145 : epoch\n",
      "\n",
      "gloss : -1922129985536.0\n",
      "wd_loss : 21219.9453125\n",
      "cls_loss : 1.9325945377349854\n",
      "acc_t : 0.212890625\n",
      "acc_s : 0.568359375\n",
      "val_loss : 2.1560715675354003\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "146 : epoch\n",
      "\n",
      "gloss : -1570956902400.0\n",
      "wd_loss : 21299.4609375\n",
      "cls_loss : 1.935119867324829\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.564453125\n",
      "val_loss : 2.157759189605713\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "147 : epoch\n",
      "\n",
      "gloss : -1149230645248.0\n",
      "wd_loss : 22642.419921875\n",
      "cls_loss : 1.929996132850647\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.5703125\n",
      "val_loss : 2.1570764064788817\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "148 : epoch\n",
      "\n",
      "gloss : -2015106039808.0\n",
      "wd_loss : 22740.265625\n",
      "cls_loss : 1.9269585609436035\n",
      "acc_t : 0.169921875\n",
      "acc_s : 0.572265625\n",
      "val_loss : 2.154264974594116\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "149 : epoch\n",
      "\n",
      "gloss : -2105331548160.0\n",
      "wd_loss : 23220.51171875\n",
      "cls_loss : 1.9162677526474\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.153484582901001\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "150 : epoch\n",
      "\n",
      "gloss : -1692163768320.0\n",
      "wd_loss : 23570.470703125\n",
      "cls_loss : 1.9166233539581299\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.53125\n",
      "val_loss : 2.1563507556915282\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "151 : epoch\n",
      "\n",
      "gloss : -1792219021312.0\n",
      "wd_loss : 24513.37109375\n",
      "cls_loss : 1.9174394607543945\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.1551399707794188\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "152 : epoch\n",
      "\n",
      "gloss : -2973696262144.0\n",
      "wd_loss : 25604.599609375\n",
      "cls_loss : 1.913676381111145\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.5546875\n",
      "val_loss : 2.154410171508789\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "153 : epoch\n",
      "\n",
      "gloss : -2751991906304.0\n",
      "wd_loss : 26485.896484375\n",
      "cls_loss : 1.9080390930175781\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.572265625\n",
      "val_loss : 2.1514245510101317\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "154 : epoch\n",
      "\n",
      "gloss : -1886681956352.0\n",
      "wd_loss : 26374.900390625\n",
      "cls_loss : 1.9113539457321167\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.5703125\n",
      "val_loss : 2.1504727363586427\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.149361085891724, epoch : 131\n",
      "\n",
      "155 : epoch\n",
      "\n",
      "gloss : -2214320930816.0\n",
      "wd_loss : 26178.6796875\n",
      "cls_loss : 1.908774971961975\n",
      "acc_t : 0.166015625\n",
      "acc_s : 0.576171875\n",
      "val_loss : 2.14386625289917\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.14386625289917, epoch : 155\n",
      "\n",
      "156 : epoch\n",
      "\n",
      "gloss : -3057182048256.0\n",
      "wd_loss : 25991.86328125\n",
      "cls_loss : 1.908228874206543\n",
      "acc_t : 0.1640625\n",
      "acc_s : 0.548828125\n",
      "val_loss : 2.143224287033081\n",
      "acc_val : 0.2125\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "157 : epoch\n",
      "\n",
      "gloss : -2795518558208.0\n",
      "wd_loss : 25522.291015625\n",
      "cls_loss : 1.9135996103286743\n",
      "acc_t : 0.16015625\n",
      "acc_s : 0.548828125\n",
      "val_loss : 2.14522500038147\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "158 : epoch\n",
      "\n",
      "gloss : -3061182365696.0\n",
      "wd_loss : 26203.9140625\n",
      "cls_loss : 1.912010669708252\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.5625\n",
      "val_loss : 2.1519975662231445\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "159 : epoch\n",
      "\n",
      "gloss : -3431894351872.0\n",
      "wd_loss : 26512.966796875\n",
      "cls_loss : 1.918594479560852\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.15055513381958\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "160 : epoch\n",
      "\n",
      "gloss : -3435669225472.0\n",
      "wd_loss : 27940.1640625\n",
      "cls_loss : 1.9081709384918213\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.1493176460266112\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "161 : epoch\n",
      "\n",
      "gloss : -3826807734272.0\n",
      "wd_loss : 28887.603515625\n",
      "cls_loss : 1.8976579904556274\n",
      "acc_t : 0.166015625\n",
      "acc_s : 0.560546875\n",
      "val_loss : 2.151854705810547\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "162 : epoch\n",
      "\n",
      "gloss : -4443195572224.0\n",
      "wd_loss : 29192.71484375\n",
      "cls_loss : 1.8948075771331787\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.5703125\n",
      "val_loss : 2.153745651245117\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "163 : epoch\n",
      "\n",
      "gloss : -4077870383104.0\n",
      "wd_loss : 29537.9296875\n",
      "cls_loss : 1.8979074954986572\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.157097005844116\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "164 : epoch\n",
      "\n",
      "gloss : -3434528899072.0\n",
      "wd_loss : 29104.671875\n",
      "cls_loss : 1.897729516029358\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.53125\n",
      "val_loss : 2.1585696220397947\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "165 : epoch\n",
      "\n",
      "gloss : -4101196480512.0\n",
      "wd_loss : 30205.875\n",
      "cls_loss : 1.896735668182373\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.5234375\n",
      "val_loss : 2.1607033252716064\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "166 : epoch\n",
      "\n",
      "gloss : -4006940508160.0\n",
      "wd_loss : 29470.626953125\n",
      "cls_loss : 1.8936926126480103\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.515625\n",
      "val_loss : 2.162567138671875\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "167 : epoch\n",
      "\n",
      "gloss : -3884011487232.0\n",
      "wd_loss : 30299.90234375\n",
      "cls_loss : 1.8919622898101807\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.521484375\n",
      "val_loss : 2.161959743499756\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "168 : epoch\n",
      "\n",
      "gloss : -3776639664128.0\n",
      "wd_loss : 30642.69921875\n",
      "cls_loss : 1.8884501457214355\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.162419891357422\n",
      "acc_val : 0.146875\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "169 : epoch\n",
      "\n",
      "gloss : -5935137292288.0\n",
      "wd_loss : 32659.73046875\n",
      "cls_loss : 1.8800443410873413\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.159854459762573\n",
      "acc_val : 0.134375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "170 : epoch\n",
      "\n",
      "gloss : -4057002934272.0\n",
      "wd_loss : 34153.73828125\n",
      "cls_loss : 1.8760958909988403\n",
      "acc_t : 0.2109375\n",
      "acc_s : 0.576171875\n",
      "val_loss : 2.163393974304199\n",
      "acc_val : 0.134375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "171 : epoch\n",
      "\n",
      "gloss : -3951036989440.0\n",
      "wd_loss : 35368.20703125\n",
      "cls_loss : 1.877915859222412\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.595703125\n",
      "val_loss : 2.167099618911743\n",
      "acc_val : 0.146875\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "172 : epoch\n",
      "\n",
      "gloss : -5207276650496.0\n",
      "wd_loss : 35793.265625\n",
      "cls_loss : 1.8742555379867554\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.60546875\n",
      "val_loss : 2.1705358028411865\n",
      "acc_val : 0.125\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "173 : epoch\n",
      "\n",
      "gloss : -5534781538304.0\n",
      "wd_loss : 36332.265625\n",
      "cls_loss : 1.8724766969680786\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.599609375\n",
      "val_loss : 2.1701300144195557\n",
      "acc_val : 0.140625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "174 : epoch\n",
      "\n",
      "gloss : -7024206151680.0\n",
      "wd_loss : 36739.52734375\n",
      "cls_loss : 1.8703861236572266\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.587890625\n",
      "val_loss : 2.167059803009033\n",
      "acc_val : 0.1375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "175 : epoch\n",
      "\n",
      "gloss : -5957604605952.0\n",
      "wd_loss : 38788.31640625\n",
      "cls_loss : 1.8706973791122437\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.564453125\n",
      "val_loss : 2.1644548892974855\n",
      "acc_val : 0.1375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "176 : epoch\n",
      "\n",
      "gloss : -8402268848128.0\n",
      "wd_loss : 39089.13671875\n",
      "cls_loss : 1.86439049243927\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.580078125\n",
      "val_loss : 2.1624027252197267\n",
      "acc_val : 0.134375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "177 : epoch\n",
      "\n",
      "gloss : -8461667532800.0\n",
      "wd_loss : 39127.26953125\n",
      "cls_loss : 1.867357850074768\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.564453125\n",
      "val_loss : 2.161898612976074\n",
      "acc_val : 0.134375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "178 : epoch\n",
      "\n",
      "gloss : -6010933084160.0\n",
      "wd_loss : 39665.13671875\n",
      "cls_loss : 1.8695685863494873\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.546875\n",
      "val_loss : 2.159753847122192\n",
      "acc_val : 0.140625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "179 : epoch\n",
      "\n",
      "gloss : -6688255508480.0\n",
      "wd_loss : 38958.8125\n",
      "cls_loss : 1.8746403455734253\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.56640625\n",
      "val_loss : 2.157110071182251\n",
      "acc_val : 0.146875\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "180 : epoch\n",
      "\n",
      "gloss : -4433171185664.0\n",
      "wd_loss : 36695.8203125\n",
      "cls_loss : 1.876915454864502\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.580078125\n",
      "val_loss : 2.1575912475585937\n",
      "acc_val : 0.1375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "181 : epoch\n",
      "\n",
      "gloss : -6166692233216.0\n",
      "wd_loss : 36488.140625\n",
      "cls_loss : 1.8735994100570679\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.59765625\n",
      "val_loss : 2.1542396545410156\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "182 : epoch\n",
      "\n",
      "gloss : -8364024135680.0\n",
      "wd_loss : 38006.828125\n",
      "cls_loss : 1.8629944324493408\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.6328125\n",
      "val_loss : 2.1487059116363527\n",
      "acc_val : 0.14375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "183 : epoch\n",
      "\n",
      "gloss : -7907026403328.0\n",
      "wd_loss : 40632.87890625\n",
      "cls_loss : 1.8561406135559082\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.640625\n",
      "val_loss : 2.149736928939819\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "184 : epoch\n",
      "\n",
      "gloss : -6282667360256.0\n",
      "wd_loss : 39268.63671875\n",
      "cls_loss : 1.8553451299667358\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.634765625\n",
      "val_loss : 2.1488452911376954\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "185 : epoch\n",
      "\n",
      "gloss : -10221775749120.0\n",
      "wd_loss : 40002.5703125\n",
      "cls_loss : 1.85867178440094\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.65234375\n",
      "val_loss : 2.1515708446502684\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "186 : epoch\n",
      "\n",
      "gloss : -8403771981824.0\n",
      "wd_loss : 39288.45703125\n",
      "cls_loss : 1.8710737228393555\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.6015625\n",
      "val_loss : 2.155258226394653\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "187 : epoch\n",
      "\n",
      "gloss : -6092070322176.0\n",
      "wd_loss : 38604.8984375\n",
      "cls_loss : 1.8725522756576538\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.591796875\n",
      "val_loss : 2.1521406173706055\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "188 : epoch\n",
      "\n",
      "gloss : -10211612950528.0\n",
      "wd_loss : 39188.51953125\n",
      "cls_loss : 1.8631781339645386\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.591796875\n",
      "val_loss : 2.146822690963745\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "189 : epoch\n",
      "\n",
      "gloss : -10057058091008.0\n",
      "wd_loss : 40231.8125\n",
      "cls_loss : 1.8581346273422241\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.55859375\n",
      "val_loss : 2.144295358657837\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "190 : epoch\n",
      "\n",
      "gloss : -9751240900608.0\n",
      "wd_loss : 42259.984375\n",
      "cls_loss : 1.8551957607269287\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.55078125\n",
      "val_loss : 2.147821617126465\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "191 : epoch\n",
      "\n",
      "gloss : -11913893249024.0\n",
      "wd_loss : 42258.15625\n",
      "cls_loss : 1.8526780605316162\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.59765625\n",
      "val_loss : 2.1532896518707276\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "192 : epoch\n",
      "\n",
      "gloss : -10124475236352.0\n",
      "wd_loss : 43558.25\n",
      "cls_loss : 1.8590624332427979\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.59375\n",
      "val_loss : 2.1554731845855715\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "193 : epoch\n",
      "\n",
      "gloss : -11878545752064.0\n",
      "wd_loss : 44187.1953125\n",
      "cls_loss : 1.8630775213241577\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.62109375\n",
      "val_loss : 2.159037399291992\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "194 : epoch\n",
      "\n",
      "gloss : -14208729088000.0\n",
      "wd_loss : 45016.703125\n",
      "cls_loss : 1.860999345779419\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.619140625\n",
      "val_loss : 2.159163570404053\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "195 : epoch\n",
      "\n",
      "gloss : -13696619249664.0\n",
      "wd_loss : 46162.17578125\n",
      "cls_loss : 1.858367919921875\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.61328125\n",
      "val_loss : 2.1562270164489745\n",
      "acc_val : 0.20625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "196 : epoch\n",
      "\n",
      "gloss : -16287608602624.0\n",
      "wd_loss : 46227.375\n",
      "cls_loss : 1.85932195186615\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.591796875\n",
      "val_loss : 2.1548977851867677\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "197 : epoch\n",
      "\n",
      "gloss : -19343851126784.0\n",
      "wd_loss : 48823.55859375\n",
      "cls_loss : 1.8544824123382568\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.59375\n",
      "val_loss : 2.148940324783325\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "198 : epoch\n",
      "\n",
      "gloss : -15810999353344.0\n",
      "wd_loss : 48856.046875\n",
      "cls_loss : 1.8544808626174927\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.609375\n",
      "val_loss : 2.14790563583374\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "199 : epoch\n",
      "\n",
      "gloss : -15276321013760.0\n",
      "wd_loss : 47647.57421875\n",
      "cls_loss : 1.8564162254333496\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.5859375\n",
      "val_loss : 2.1458926677703856\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.143224287033081, epoch : 156\n",
      "\n",
      "200 : epoch\n",
      "\n",
      "gloss : -9959752335360.0\n",
      "wd_loss : 49905.16015625\n",
      "cls_loss : 1.849515676498413\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.587890625\n",
      "val_loss : 2.136466550827026\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.136466550827026, epoch : 200\n",
      "\n",
      "201 : epoch\n",
      "\n",
      "gloss : -11085062078464.0\n",
      "wd_loss : 50539.1953125\n",
      "cls_loss : 1.8479642868041992\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.6015625\n",
      "val_loss : 2.131668281555176\n",
      "acc_val : 0.203125\n",
      "best_val_loss : 2.131668281555176, epoch : 201\n",
      "\n",
      "202 : epoch\n",
      "\n",
      "gloss : -21021826482176.0\n",
      "wd_loss : 51638.9296875\n",
      "cls_loss : 1.8388653993606567\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.6015625\n",
      "val_loss : 2.1339515209198\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.131668281555176, epoch : 201\n",
      "\n",
      "203 : epoch\n",
      "\n",
      "gloss : -20796747546624.0\n",
      "wd_loss : 50630.3359375\n",
      "cls_loss : 1.84193754196167\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.5859375\n",
      "val_loss : 2.135332155227661\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.131668281555176, epoch : 201\n",
      "\n",
      "204 : epoch\n",
      "\n",
      "gloss : -17084747612160.0\n",
      "wd_loss : 53193.421875\n",
      "cls_loss : 1.8375599384307861\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.6015625\n",
      "val_loss : 2.1387520313262938\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.131668281555176, epoch : 201\n",
      "\n",
      "205 : epoch\n",
      "\n",
      "gloss : -20879146745856.0\n",
      "wd_loss : 51313.01171875\n",
      "cls_loss : 1.8405143022537231\n",
      "acc_t : 0.203125\n",
      "acc_s : 0.6015625\n",
      "val_loss : 2.13716344833374\n",
      "acc_val : 0.221875\n",
      "best_val_loss : 2.131668281555176, epoch : 201\n",
      "\n",
      "206 : epoch\n",
      "\n",
      "gloss : -19603195428864.0\n",
      "wd_loss : 50577.2109375\n",
      "cls_loss : 1.8395973443984985\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.603515625\n",
      "val_loss : 2.135309410095215\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.131668281555176, epoch : 201\n",
      "\n",
      "207 : epoch\n",
      "\n",
      "gloss : -20497794334720.0\n",
      "wd_loss : 53008.86328125\n",
      "cls_loss : 1.837834119796753\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.60546875\n",
      "val_loss : 2.133728551864624\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.131668281555176, epoch : 201\n",
      "\n",
      "208 : epoch\n",
      "\n",
      "gloss : -15619171811328.0\n",
      "wd_loss : 51782.0546875\n",
      "cls_loss : 1.8361566066741943\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.607421875\n",
      "val_loss : 2.1313164234161377\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "209 : epoch\n",
      "\n",
      "gloss : -18623659769856.0\n",
      "wd_loss : 50553.765625\n",
      "cls_loss : 1.8349190950393677\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.611328125\n",
      "val_loss : 2.13287992477417\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "210 : epoch\n",
      "\n",
      "gloss : -25339820507136.0\n",
      "wd_loss : 50059.95703125\n",
      "cls_loss : 1.8366117477416992\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.615234375\n",
      "val_loss : 2.135665941238403\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "211 : epoch\n",
      "\n",
      "gloss : -23939550019584.0\n",
      "wd_loss : 53774.11328125\n",
      "cls_loss : 1.8451011180877686\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.6015625\n",
      "val_loss : 2.1381505012512205\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "212 : epoch\n",
      "\n",
      "gloss : -25886654988288.0\n",
      "wd_loss : 50391.359375\n",
      "cls_loss : 1.8467621803283691\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.6015625\n",
      "val_loss : 2.141224479675293\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "213 : epoch\n",
      "\n",
      "gloss : -28102184927232.0\n",
      "wd_loss : 53405.17578125\n",
      "cls_loss : 1.8437676429748535\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.609375\n",
      "val_loss : 2.1418850898742674\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "214 : epoch\n",
      "\n",
      "gloss : -25863942832128.0\n",
      "wd_loss : 57108.71875\n",
      "cls_loss : 1.838226556777954\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.59765625\n",
      "val_loss : 2.141065979003906\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "215 : epoch\n",
      "\n",
      "gloss : -25054666555392.0\n",
      "wd_loss : 58311.9140625\n",
      "cls_loss : 1.8337758779525757\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.6171875\n",
      "val_loss : 2.1430456161499025\n",
      "acc_val : 0.15625\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "216 : epoch\n",
      "\n",
      "gloss : -24061482631168.0\n",
      "wd_loss : 57616.13671875\n",
      "cls_loss : 1.8363655805587769\n",
      "acc_t : 0.162109375\n",
      "acc_s : 0.595703125\n",
      "val_loss : 2.1428425312042236\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "217 : epoch\n",
      "\n",
      "gloss : -29762623897600.0\n",
      "wd_loss : 58888.375\n",
      "cls_loss : 1.8415470123291016\n",
      "acc_t : 0.166015625\n",
      "acc_s : 0.6171875\n",
      "val_loss : 2.142572450637817\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "218 : epoch\n",
      "\n",
      "gloss : -27020291473408.0\n",
      "wd_loss : 57627.5\n",
      "cls_loss : 1.8376983404159546\n",
      "acc_t : 0.162109375\n",
      "acc_s : 0.615234375\n",
      "val_loss : 2.1380423069000245\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "219 : epoch\n",
      "\n",
      "gloss : -35301166678016.0\n",
      "wd_loss : 58839.640625\n",
      "cls_loss : 1.831206202507019\n",
      "acc_t : 0.162109375\n",
      "acc_s : 0.619140625\n",
      "val_loss : 2.133539009094238\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "220 : epoch\n",
      "\n",
      "gloss : -27852183437312.0\n",
      "wd_loss : 58403.9375\n",
      "cls_loss : 1.830069661140442\n",
      "acc_t : 0.16015625\n",
      "acc_s : 0.623046875\n",
      "val_loss : 2.132757139205933\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "221 : epoch\n",
      "\n",
      "gloss : -28243094667264.0\n",
      "wd_loss : 58134.5859375\n",
      "cls_loss : 1.8274126052856445\n",
      "acc_t : 0.158203125\n",
      "acc_s : 0.603515625\n",
      "val_loss : 2.131557273864746\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "222 : epoch\n",
      "\n",
      "gloss : -33249768243200.0\n",
      "wd_loss : 60215.9296875\n",
      "cls_loss : 1.8308216333389282\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.58984375\n",
      "val_loss : 2.1316604137420656\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1313164234161377, epoch : 208\n",
      "\n",
      "223 : epoch\n",
      "\n",
      "gloss : -23926577037312.0\n",
      "wd_loss : 63541.328125\n",
      "cls_loss : 1.832427978515625\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.59765625\n",
      "val_loss : 2.1285603046417236\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "224 : epoch\n",
      "\n",
      "gloss : -38512770416640.0\n",
      "wd_loss : 61059.6015625\n",
      "cls_loss : 1.836622953414917\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.611328125\n",
      "val_loss : 2.128867483139038\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "225 : epoch\n",
      "\n",
      "gloss : -30345189654528.0\n",
      "wd_loss : 63353.19140625\n",
      "cls_loss : 1.8322346210479736\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.62109375\n",
      "val_loss : 2.13341965675354\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "226 : epoch\n",
      "\n",
      "gloss : -36011321065472.0\n",
      "wd_loss : 61509.8046875\n",
      "cls_loss : 1.8424608707427979\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.599609375\n",
      "val_loss : 2.1371841430664062\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "227 : epoch\n",
      "\n",
      "gloss : -38721487372288.0\n",
      "wd_loss : 63791.83984375\n",
      "cls_loss : 1.8380783796310425\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.626953125\n",
      "val_loss : 2.1411884784698487\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "228 : epoch\n",
      "\n",
      "gloss : -37215690293248.0\n",
      "wd_loss : 65543.484375\n",
      "cls_loss : 1.8308249711990356\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.587890625\n",
      "val_loss : 2.1402100563049316\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "229 : epoch\n",
      "\n",
      "gloss : -40796669607936.0\n",
      "wd_loss : 69008.3515625\n",
      "cls_loss : 1.8237690925598145\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.591796875\n",
      "val_loss : 2.141044282913208\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "230 : epoch\n",
      "\n",
      "gloss : -32883297222656.0\n",
      "wd_loss : 68614.6640625\n",
      "cls_loss : 1.8223865032196045\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.587890625\n",
      "val_loss : 2.1424702167510987\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "231 : epoch\n",
      "\n",
      "gloss : -43933556015104.0\n",
      "wd_loss : 71700.734375\n",
      "cls_loss : 1.8281729221343994\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.591796875\n",
      "val_loss : 2.1487853050231935\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "232 : epoch\n",
      "\n",
      "gloss : -51210543431680.0\n",
      "wd_loss : 71001.9921875\n",
      "cls_loss : 1.8301568031311035\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.59765625\n",
      "val_loss : 2.149764394760132\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "233 : epoch\n",
      "\n",
      "gloss : -42150976815104.0\n",
      "wd_loss : 71698.15625\n",
      "cls_loss : 1.832509160041809\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.595703125\n",
      "val_loss : 2.150840950012207\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "234 : epoch\n",
      "\n",
      "gloss : -39189370372096.0\n",
      "wd_loss : 71790.28125\n",
      "cls_loss : 1.8314756155014038\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.619140625\n",
      "val_loss : 2.150429439544678\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "235 : epoch\n",
      "\n",
      "gloss : -57241923223552.0\n",
      "wd_loss : 72760.7578125\n",
      "cls_loss : 1.8277424573898315\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.626953125\n",
      "val_loss : 2.151905632019043\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "236 : epoch\n",
      "\n",
      "gloss : -59377834786816.0\n",
      "wd_loss : 71137.0078125\n",
      "cls_loss : 1.8175938129425049\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.619140625\n",
      "val_loss : 2.1478552341461183\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "237 : epoch\n",
      "\n",
      "gloss : -51812321198080.0\n",
      "wd_loss : 72767.34375\n",
      "cls_loss : 1.821494221687317\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.61328125\n",
      "val_loss : 2.1430564880371095\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "238 : epoch\n",
      "\n",
      "gloss : -59269391056896.0\n",
      "wd_loss : 72122.90625\n",
      "cls_loss : 1.8154644966125488\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.6484375\n",
      "val_loss : 2.1385622024536133\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "239 : epoch\n",
      "\n",
      "gloss : -63088103522304.0\n",
      "wd_loss : 73095.984375\n",
      "cls_loss : 1.8143887519836426\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.6640625\n",
      "val_loss : 2.143222522735596\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "240 : epoch\n",
      "\n",
      "gloss : -68635624210432.0\n",
      "wd_loss : 75416.71875\n",
      "cls_loss : 1.8159537315368652\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.669921875\n",
      "val_loss : 2.1449536800384523\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "241 : epoch\n",
      "\n",
      "gloss : -49907113132032.0\n",
      "wd_loss : 76731.0546875\n",
      "cls_loss : 1.8133525848388672\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.65234375\n",
      "val_loss : 2.1466281414031982\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "242 : epoch\n",
      "\n",
      "gloss : -54097134747648.0\n",
      "wd_loss : 82532.8828125\n",
      "cls_loss : 1.805949091911316\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.638671875\n",
      "val_loss : 2.1475409507751464\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "243 : epoch\n",
      "\n",
      "gloss : -49667161194496.0\n",
      "wd_loss : 80528.8515625\n",
      "cls_loss : 1.7999906539916992\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.671875\n",
      "val_loss : 2.1416452884674073\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "244 : epoch\n",
      "\n",
      "gloss : -76635982266368.0\n",
      "wd_loss : 83356.90625\n",
      "cls_loss : 1.7926750183105469\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.65234375\n",
      "val_loss : 2.1427196979522707\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "245 : epoch\n",
      "\n",
      "gloss : -73926965198848.0\n",
      "wd_loss : 83765.703125\n",
      "cls_loss : 1.7926561832427979\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.640625\n",
      "val_loss : 2.1431904315948485\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "246 : epoch\n",
      "\n",
      "gloss : -47137446428672.0\n",
      "wd_loss : 83867.8828125\n",
      "cls_loss : 1.788698673248291\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.662109375\n",
      "val_loss : 2.1446794509887694\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "247 : epoch\n",
      "\n",
      "gloss : -58837071560704.0\n",
      "wd_loss : 86825.078125\n",
      "cls_loss : 1.7818551063537598\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.677734375\n",
      "val_loss : 2.1482340812683107\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "248 : epoch\n",
      "\n",
      "gloss : -70149247860736.0\n",
      "wd_loss : 83821.4296875\n",
      "cls_loss : 1.786871075630188\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.65234375\n",
      "val_loss : 2.1456281661987306\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "249 : epoch\n",
      "\n",
      "gloss : -74024826699776.0\n",
      "wd_loss : 83194.375\n",
      "cls_loss : 1.7884702682495117\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.64453125\n",
      "val_loss : 2.1421857833862306\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "250 : epoch\n",
      "\n",
      "gloss : -62480172711936.0\n",
      "wd_loss : 84398.140625\n",
      "cls_loss : 1.784875512123108\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.65625\n",
      "val_loss : 2.1471059799194334\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "251 : epoch\n",
      "\n",
      "gloss : -80266110435328.0\n",
      "wd_loss : 83778.28125\n",
      "cls_loss : 1.7788116931915283\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.67578125\n",
      "val_loss : 2.150161361694336\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "252 : epoch\n",
      "\n",
      "gloss : -76817679515648.0\n",
      "wd_loss : 84287.34375\n",
      "cls_loss : 1.7812025547027588\n",
      "acc_t : 0.197265625\n",
      "acc_s : 0.669921875\n",
      "val_loss : 2.1474228382110594\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "253 : epoch\n",
      "\n",
      "gloss : -110138908213248.0\n",
      "wd_loss : 82120.96875\n",
      "cls_loss : 1.78800368309021\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.64453125\n",
      "val_loss : 2.1413199424743654\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "254 : epoch\n",
      "\n",
      "gloss : -75505533452288.0\n",
      "wd_loss : 81072.1015625\n",
      "cls_loss : 1.7838457822799683\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.6640625\n",
      "val_loss : 2.1410805225372314\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "255 : epoch\n",
      "\n",
      "gloss : -73401519570944.0\n",
      "wd_loss : 82024.28125\n",
      "cls_loss : 1.7897354364395142\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.6484375\n",
      "val_loss : 2.140483856201172\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "256 : epoch\n",
      "\n",
      "gloss : -95183496544256.0\n",
      "wd_loss : 87562.546875\n",
      "cls_loss : 1.7906525135040283\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.6640625\n",
      "val_loss : 2.143271970748901\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "257 : epoch\n",
      "\n",
      "gloss : -65174086090752.0\n",
      "wd_loss : 88072.6953125\n",
      "cls_loss : 1.7872573137283325\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.65625\n",
      "val_loss : 2.141368341445923\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "258 : epoch\n",
      "\n",
      "gloss : -85968988143616.0\n",
      "wd_loss : 88717.6171875\n",
      "cls_loss : 1.7855054140090942\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.669921875\n",
      "val_loss : 2.1421151638031004\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "259 : epoch\n",
      "\n",
      "gloss : -95025178345472.0\n",
      "wd_loss : 90687.9921875\n",
      "cls_loss : 1.7774298191070557\n",
      "acc_t : 0.158203125\n",
      "acc_s : 0.701171875\n",
      "val_loss : 2.140995740890503\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "260 : epoch\n",
      "\n",
      "gloss : -94874879655936.0\n",
      "wd_loss : 95392.8046875\n",
      "cls_loss : 1.7765052318572998\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.658203125\n",
      "val_loss : 2.139116811752319\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "261 : epoch\n",
      "\n",
      "gloss : -100614205865984.0\n",
      "wd_loss : 96939.0625\n",
      "cls_loss : 1.770749568939209\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.681640625\n",
      "val_loss : 2.138978052139282\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "262 : epoch\n",
      "\n",
      "gloss : -114196821835776.0\n",
      "wd_loss : 95220.9140625\n",
      "cls_loss : 1.7746021747589111\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.67578125\n",
      "val_loss : 2.140126371383667\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "263 : epoch\n",
      "\n",
      "gloss : -92788297302016.0\n",
      "wd_loss : 94299.0546875\n",
      "cls_loss : 1.7753735780715942\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.666015625\n",
      "val_loss : 2.1386255741119387\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "264 : epoch\n",
      "\n",
      "gloss : -104097080410112.0\n",
      "wd_loss : 90651.96875\n",
      "cls_loss : 1.7816654443740845\n",
      "acc_t : 0.232421875\n",
      "acc_s : 0.642578125\n",
      "val_loss : 2.1400264739990233\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "265 : epoch\n",
      "\n",
      "gloss : -82816356319232.0\n",
      "wd_loss : 92989.984375\n",
      "cls_loss : 1.7798868417739868\n",
      "acc_t : 0.21484375\n",
      "acc_s : 0.65234375\n",
      "val_loss : 2.139496183395386\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "266 : epoch\n",
      "\n",
      "gloss : -131624675049472.0\n",
      "wd_loss : 93459.9375\n",
      "cls_loss : 1.772292971611023\n",
      "acc_t : 0.232421875\n",
      "acc_s : 0.658203125\n",
      "val_loss : 2.140797185897827\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "267 : epoch\n",
      "\n",
      "gloss : -81234734612480.0\n",
      "wd_loss : 95700.484375\n",
      "cls_loss : 1.768083095550537\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.671875\n",
      "val_loss : 2.1387115001678465\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "268 : epoch\n",
      "\n",
      "gloss : -94180562960384.0\n",
      "wd_loss : 96618.6875\n",
      "cls_loss : 1.767500400543213\n",
      "acc_t : 0.224609375\n",
      "acc_s : 0.68359375\n",
      "val_loss : 2.1375380516052247\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "269 : epoch\n",
      "\n",
      "gloss : -132433882120192.0\n",
      "wd_loss : 97992.484375\n",
      "cls_loss : 1.7640670537948608\n",
      "acc_t : 0.19921875\n",
      "acc_s : 0.685546875\n",
      "val_loss : 2.133280372619629\n",
      "acc_val : 0.190625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "270 : epoch\n",
      "\n",
      "gloss : -145170431475712.0\n",
      "wd_loss : 102579.125\n",
      "cls_loss : 1.7586296796798706\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.724609375\n",
      "val_loss : 2.1335920810699465\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "271 : epoch\n",
      "\n",
      "gloss : -120059670298624.0\n",
      "wd_loss : 105196.859375\n",
      "cls_loss : 1.7599543333053589\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.70703125\n",
      "val_loss : 2.1386508464813234\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "272 : epoch\n",
      "\n",
      "gloss : -167488205619200.0\n",
      "wd_loss : 106787.6171875\n",
      "cls_loss : 1.7644522190093994\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.705078125\n",
      "val_loss : 2.1484072685241697\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "273 : epoch\n",
      "\n",
      "gloss : -142681263046656.0\n",
      "wd_loss : 102619.3984375\n",
      "cls_loss : 1.7692762613296509\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.703125\n",
      "val_loss : 2.1511058807373047\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "274 : epoch\n",
      "\n",
      "gloss : -126157886324736.0\n",
      "wd_loss : 100948.5625\n",
      "cls_loss : 1.7684355974197388\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.6953125\n",
      "val_loss : 2.1533021450042726\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "275 : epoch\n",
      "\n",
      "gloss : -131930498531328.0\n",
      "wd_loss : 104542.15625\n",
      "cls_loss : 1.7699055671691895\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.697265625\n",
      "val_loss : 2.1576523780822754\n",
      "acc_val : 0.171875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "276 : epoch\n",
      "\n",
      "gloss : -127374737801216.0\n",
      "wd_loss : 105158.75\n",
      "cls_loss : 1.767883062362671\n",
      "acc_t : 0.208984375\n",
      "acc_s : 0.689453125\n",
      "val_loss : 2.158148193359375\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "277 : epoch\n",
      "\n",
      "gloss : -168931247521792.0\n",
      "wd_loss : 112106.515625\n",
      "cls_loss : 1.7600784301757812\n",
      "acc_t : 0.20703125\n",
      "acc_s : 0.6796875\n",
      "val_loss : 2.160556364059448\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "278 : epoch\n",
      "\n",
      "gloss : -154672090316800.0\n",
      "wd_loss : 109182.6875\n",
      "cls_loss : 1.7635310888290405\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.669921875\n",
      "val_loss : 2.1666619300842287\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "279 : epoch\n",
      "\n",
      "gloss : -186999638065152.0\n",
      "wd_loss : 108010.5625\n",
      "cls_loss : 1.7646418809890747\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.69140625\n",
      "val_loss : 2.1648256301879885\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "280 : epoch\n",
      "\n",
      "gloss : -114356616429568.0\n",
      "wd_loss : 108682.0\n",
      "cls_loss : 1.7608586549758911\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.701171875\n",
      "val_loss : 2.1617273807525637\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "281 : epoch\n",
      "\n",
      "gloss : -158460587016192.0\n",
      "wd_loss : 113386.6171875\n",
      "cls_loss : 1.7601090669631958\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.69921875\n",
      "val_loss : 2.1626251220703123\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "282 : epoch\n",
      "\n",
      "gloss : -180397887455232.0\n",
      "wd_loss : 112792.5\n",
      "cls_loss : 1.7560269832611084\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.7265625\n",
      "val_loss : 2.164459705352783\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "283 : epoch\n",
      "\n",
      "gloss : -206693220745216.0\n",
      "wd_loss : 108684.5859375\n",
      "cls_loss : 1.7692965269088745\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.703125\n",
      "val_loss : 2.163428258895874\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "284 : epoch\n",
      "\n",
      "gloss : -183995711094784.0\n",
      "wd_loss : 107430.78125\n",
      "cls_loss : 1.769098162651062\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.720703125\n",
      "val_loss : 2.1605485916137694\n",
      "acc_val : 0.2\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "285 : epoch\n",
      "\n",
      "gloss : -160541062463488.0\n",
      "wd_loss : 101442.8125\n",
      "cls_loss : 1.7771825790405273\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.681640625\n",
      "val_loss : 2.1558276653289794\n",
      "acc_val : 0.196875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "286 : epoch\n",
      "\n",
      "gloss : -172205354778624.0\n",
      "wd_loss : 102887.203125\n",
      "cls_loss : 1.7764524221420288\n",
      "acc_t : 0.201171875\n",
      "acc_s : 0.6875\n",
      "val_loss : 2.155654716491699\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "287 : epoch\n",
      "\n",
      "gloss : -75319776116736.0\n",
      "wd_loss : 105610.046875\n",
      "cls_loss : 1.7676771879196167\n",
      "acc_t : 0.216796875\n",
      "acc_s : 0.71484375\n",
      "val_loss : 2.151913118362427\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "288 : epoch\n",
      "\n",
      "gloss : -221714097111040.0\n",
      "wd_loss : 107815.375\n",
      "cls_loss : 1.768505334854126\n",
      "acc_t : 0.205078125\n",
      "acc_s : 0.7109375\n",
      "val_loss : 2.1523178100585936\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "289 : epoch\n",
      "\n",
      "gloss : -174317740490752.0\n",
      "wd_loss : 112918.796875\n",
      "cls_loss : 1.763988733291626\n",
      "acc_t : 0.1953125\n",
      "acc_s : 0.712890625\n",
      "val_loss : 2.151083469390869\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "290 : epoch\n",
      "\n",
      "gloss : -237401381273600.0\n",
      "wd_loss : 117710.15625\n",
      "cls_loss : 1.7571465969085693\n",
      "acc_t : 0.205078125\n",
      "acc_s : 0.705078125\n",
      "val_loss : 2.154248046875\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "291 : epoch\n",
      "\n",
      "gloss : -194129803870208.0\n",
      "wd_loss : 117243.546875\n",
      "cls_loss : 1.7621591091156006\n",
      "acc_t : 0.189453125\n",
      "acc_s : 0.708984375\n",
      "val_loss : 2.1530313014984133\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "292 : epoch\n",
      "\n",
      "gloss : -205390100824064.0\n",
      "wd_loss : 119700.5\n",
      "cls_loss : 1.75459885597229\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.720703125\n",
      "val_loss : 2.1504945278167726\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "293 : epoch\n",
      "\n",
      "gloss : -265313971077120.0\n",
      "wd_loss : 120652.546875\n",
      "cls_loss : 1.753048062324524\n",
      "acc_t : 0.1875\n",
      "acc_s : 0.712890625\n",
      "val_loss : 2.154113435745239\n",
      "acc_val : 0.15625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "294 : epoch\n",
      "\n",
      "gloss : -246828532498432.0\n",
      "wd_loss : 124608.703125\n",
      "cls_loss : 1.7504945993423462\n",
      "acc_t : 0.185546875\n",
      "acc_s : 0.703125\n",
      "val_loss : 2.1512770652770996\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "295 : epoch\n",
      "\n",
      "gloss : -195161954975744.0\n",
      "wd_loss : 120615.390625\n",
      "cls_loss : 1.7535704374313354\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.708984375\n",
      "val_loss : 2.148854684829712\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "296 : epoch\n",
      "\n",
      "gloss : -262603678941184.0\n",
      "wd_loss : 118752.1328125\n",
      "cls_loss : 1.7528740167617798\n",
      "acc_t : 0.173828125\n",
      "acc_s : 0.703125\n",
      "val_loss : 2.1456291675567627\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "297 : epoch\n",
      "\n",
      "gloss : -231346047811584.0\n",
      "wd_loss : 120968.1484375\n",
      "cls_loss : 1.7558361291885376\n",
      "acc_t : 0.169921875\n",
      "acc_s : 0.720703125\n",
      "val_loss : 2.147206449508667\n",
      "acc_val : 0.140625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "298 : epoch\n",
      "\n",
      "gloss : -267801109463040.0\n",
      "wd_loss : 120711.234375\n",
      "cls_loss : 1.7490603923797607\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.7265625\n",
      "val_loss : 2.1504189491271974\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "299 : epoch\n",
      "\n",
      "gloss : -271324173828096.0\n",
      "wd_loss : 124276.265625\n",
      "cls_loss : 1.7473644018173218\n",
      "acc_t : 0.181640625\n",
      "acc_s : 0.75390625\n",
      "val_loss : 2.1523044109344482\n",
      "acc_val : 0.15625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "300 : epoch\n",
      "\n",
      "gloss : -286415430615040.0\n",
      "wd_loss : 129433.59375\n",
      "cls_loss : 1.7494878768920898\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.751953125\n",
      "val_loss : 2.151887226104736\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "301 : epoch\n",
      "\n",
      "gloss : -259175607173120.0\n",
      "wd_loss : 131340.578125\n",
      "cls_loss : 1.7394208908081055\n",
      "acc_t : 0.171875\n",
      "acc_s : 0.779296875\n",
      "val_loss : 2.1524251461029054\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "302 : epoch\n",
      "\n",
      "gloss : -291506644582400.0\n",
      "wd_loss : 141131.125\n",
      "cls_loss : 1.7327626943588257\n",
      "acc_t : 0.171875\n",
      "acc_s : 0.767578125\n",
      "val_loss : 2.1526803016662597\n",
      "acc_val : 0.146875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "303 : epoch\n",
      "\n",
      "gloss : -314221787086848.0\n",
      "wd_loss : 134100.625\n",
      "cls_loss : 1.734269618988037\n",
      "acc_t : 0.162109375\n",
      "acc_s : 0.779296875\n",
      "val_loss : 2.146619701385498\n",
      "acc_val : 0.15\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "304 : epoch\n",
      "\n",
      "gloss : -255444622770176.0\n",
      "wd_loss : 136327.625\n",
      "cls_loss : 1.7291020154953003\n",
      "acc_t : 0.15625\n",
      "acc_s : 0.744140625\n",
      "val_loss : 2.1485033988952638\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "305 : epoch\n",
      "\n",
      "gloss : -270375757807616.0\n",
      "wd_loss : 132989.578125\n",
      "cls_loss : 1.7325150966644287\n",
      "acc_t : 0.16796875\n",
      "acc_s : 0.771484375\n",
      "val_loss : 2.1500386714935305\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "306 : epoch\n",
      "\n",
      "gloss : -277830428524544.0\n",
      "wd_loss : 136331.4375\n",
      "cls_loss : 1.7321357727050781\n",
      "acc_t : 0.16796875\n",
      "acc_s : 0.7578125\n",
      "val_loss : 2.154353618621826\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "307 : epoch\n",
      "\n",
      "gloss : -329691823079424.0\n",
      "wd_loss : 131062.75\n",
      "cls_loss : 1.7322914600372314\n",
      "acc_t : 0.17578125\n",
      "acc_s : 0.775390625\n",
      "val_loss : 2.16018967628479\n",
      "acc_val : 0.19375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "308 : epoch\n",
      "\n",
      "gloss : -314326074261504.0\n",
      "wd_loss : 127041.421875\n",
      "cls_loss : 1.7329741716384888\n",
      "acc_t : 0.177734375\n",
      "acc_s : 0.767578125\n",
      "val_loss : 2.1606187343597414\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "309 : epoch\n",
      "\n",
      "gloss : -293672952266752.0\n",
      "wd_loss : 127744.03125\n",
      "cls_loss : 1.732303261756897\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.77734375\n",
      "val_loss : 2.158598613739014\n",
      "acc_val : 0.184375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "310 : epoch\n",
      "\n",
      "gloss : -297126206636032.0\n",
      "wd_loss : 132524.515625\n",
      "cls_loss : 1.732622504234314\n",
      "acc_t : 0.169921875\n",
      "acc_s : 0.74609375\n",
      "val_loss : 2.1610278129577636\n",
      "acc_val : 0.1875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "311 : epoch\n",
      "\n",
      "gloss : -317197025017856.0\n",
      "wd_loss : 126050.15625\n",
      "cls_loss : 1.7288568019866943\n",
      "acc_t : 0.158203125\n",
      "acc_s : 0.755859375\n",
      "val_loss : 2.164315176010132\n",
      "acc_val : 0.178125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "312 : epoch\n",
      "\n",
      "gloss : -248426847535104.0\n",
      "wd_loss : 137839.234375\n",
      "cls_loss : 1.7338086366653442\n",
      "acc_t : 0.166015625\n",
      "acc_s : 0.744140625\n",
      "val_loss : 2.159291982650757\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "313 : epoch\n",
      "\n",
      "gloss : -289121696219136.0\n",
      "wd_loss : 140813.640625\n",
      "cls_loss : 1.7375510931015015\n",
      "acc_t : 0.158203125\n",
      "acc_s : 0.75390625\n",
      "val_loss : 2.1621220111846924\n",
      "acc_val : 0.153125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "314 : epoch\n",
      "\n",
      "gloss : -285790647091200.0\n",
      "wd_loss : 145682.0\n",
      "cls_loss : 1.7379399538040161\n",
      "acc_t : 0.1640625\n",
      "acc_s : 0.755859375\n",
      "val_loss : 2.163178825378418\n",
      "acc_val : 0.159375\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "315 : epoch\n",
      "\n",
      "gloss : -425670752600064.0\n",
      "wd_loss : 150510.28125\n",
      "cls_loss : 1.7366282939910889\n",
      "acc_t : 0.150390625\n",
      "acc_s : 0.73828125\n",
      "val_loss : 2.158141279220581\n",
      "acc_val : 0.15625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "316 : epoch\n",
      "\n",
      "gloss : -436700362833920.0\n",
      "wd_loss : 151828.0\n",
      "cls_loss : 1.7328251600265503\n",
      "acc_t : 0.154296875\n",
      "acc_s : 0.759765625\n",
      "val_loss : 2.1557498931884767\n",
      "acc_val : 0.1625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "317 : epoch\n",
      "\n",
      "gloss : -341782827106304.0\n",
      "wd_loss : 152990.96875\n",
      "cls_loss : 1.73253333568573\n",
      "acc_t : 0.1640625\n",
      "acc_s : 0.75390625\n",
      "val_loss : 2.1544069766998293\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "318 : epoch\n",
      "\n",
      "gloss : -371104333955072.0\n",
      "wd_loss : 147599.03125\n",
      "cls_loss : 1.7315537929534912\n",
      "acc_t : 0.16015625\n",
      "acc_s : 0.744140625\n",
      "val_loss : 2.153582239151001\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "319 : epoch\n",
      "\n",
      "gloss : -416518143737856.0\n",
      "wd_loss : 142199.734375\n",
      "cls_loss : 1.738210916519165\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.732421875\n",
      "val_loss : 2.155372476577759\n",
      "acc_val : 0.18125\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "320 : epoch\n",
      "\n",
      "gloss : -304580558585856.0\n",
      "wd_loss : 142170.3125\n",
      "cls_loss : 1.7442688941955566\n",
      "acc_t : 0.18359375\n",
      "acc_s : 0.70703125\n",
      "val_loss : 2.152852487564087\n",
      "acc_val : 0.175\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "321 : epoch\n",
      "\n",
      "gloss : -399179293655040.0\n",
      "wd_loss : 139208.15625\n",
      "cls_loss : 1.7369707822799683\n",
      "acc_t : 0.193359375\n",
      "acc_s : 0.70703125\n",
      "val_loss : 2.157424259185791\n",
      "acc_val : 0.165625\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "322 : epoch\n",
      "\n",
      "gloss : -401280136642560.0\n",
      "wd_loss : 141409.609375\n",
      "cls_loss : 1.7288343906402588\n",
      "acc_t : 0.19140625\n",
      "acc_s : 0.7265625\n",
      "val_loss : 2.1592608451843263\n",
      "acc_val : 0.16875\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n",
      "\n",
      "323 : epoch\n",
      "\n",
      "gloss : -399582919917568.0\n",
      "wd_loss : 147193.328125\n",
      "cls_loss : 1.7271039485931396\n",
      "acc_t : 0.1796875\n",
      "acc_s : 0.755859375\n",
      "val_loss : 2.1638246536254884\n",
      "acc_val : 0.15625\n",
      "\n",
      "accuracy_t : 0.19181743421052633\n",
      "accuracy_s : 0.5997303115325078\n",
      "accuracy_val : 0.17656733746130038\n",
      "best_val_loss : 2.1285603046417236, epoch : 223\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fe_ARO, dis_ARO, cls_ARO = train_val(source_ARO, target_ARO, val_ARO, 'AROUSAL', 1000, 10, 10, 10, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "ecg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
